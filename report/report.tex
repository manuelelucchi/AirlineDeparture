%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	letterpaper, % Paper size, specify a4paper (A4) or letterpaper (US letter)
	10pt, % Default font size, specify 10pt, 11pt or 12pt
]{class}

\usepackage{caption}

\addbibresource{bibliography.bib} % Bibliography file (located in the same folder as the template)

%----------------------------------------------------------------------------------------
%	REPORT INFORMATION
%----------------------------------------------------------------------------------------

\title{Airline Departure\\Data Analysis and Regression} % Report title

\author{Lucchi Manuele \& Tricella Davide} % Author name(s), add additional authors like: '\& James \textsc{Smith}'

\date{\today} % Date of the report

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert the title, author and date using the information specified above

\begin{center}
	\begin{tabular}{l r}
		Instructors: Professor \textsc{Cesa-Bianchi} \& Professor \textsc{Malchiodi}
	\end{tabular}
\end{center}

%----------------------------------------------------------------------------------------
%	DECLARATION
%----------------------------------------------------------------------------------------

\textit{We declare that this material,
	which we now submit for assessment, is entirely our own work and has not been
	taken from the work of others, save and to the extent that such work has been cited and
	acknowledged within the text of our work. We understand that plagiarism, collusion,
	and copying are grave and serious offences in the university and accept the penalties that
	would be imposed should I engage in plagiarism, collusion or copying. This assignment,
	or any part of it, has not been previously submitted by us or any other person for
	assessment on this or any other course of study.}

\tableofcontents

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}
	The purpose of this paper is to evaluate the usage of a Logistic Regression model on a airlines dataset to predict flight cancellation or diversion, in a scalable and time/space efficient implementation.
\end{abstract}

\section{Definitions}\label{definitions} % Labels provide a point for referencing, in this case with \ref{definitions} to refer to this subsection number

\begin{description}
	\item[Label]
	\item[Model]
\end{description}

%----------------------------------------------------------------------------------------
%	DATASET
%----------------------------------------------------------------------------------------

\section{Dataset}

The initial dataset, [Airline Delay and Cancellation Data] CITAZIONE is made of 9 years of airlines flights data, composed by 10 files (one for each year from 2009 to 2018) of around 6 milions records each.
The files presents 28 columns, of which we only took the 9 more relevant\\

\begin{description}
	\item[FL\_DATE] The flight date.
	\item[OP\_CARRIER] The carrier code.
	\item[ORIGIN] The departure place.
	\item[DEST] The destination place.
	\item[CRS\_DEP\_TIME] The.
	\item[CRS\_ARR\_TIME] The.
	\item[CANCELLED] If the flight has been canceled.
	\item[DIVERTED] If the flight has been diverted.
	\item[CRS\_ELAPSED\_TIME] TODO
	\item[DISTANCE] The distance the flight has to cover.\\
\end{description}

In the case the prediction is about the cancellation, the DIVERTED column will be ignored, while if the prediction is on if the flight would be diverted or not, the CANCELLED column will be ignored.\\
The carrier code is a two characters alphanumeric code, the origin and destination places are a three characters alphanumeric code.\\
Flight date, departure time and arrival time are dates, while the elapsed time and the distance are real numbers.\\
Cancelled and diverted are either 0 or 1.\\

DIRE PERCHE' ABBIAMO ESCLUSO ALCUNE COLONNE

One milion of records equally distributed between the files were taken to perform the training.

%----------------------------------------------------------------------------------------
%	PREPROCESSING
%----------------------------------------------------------------------------------------

\section{Preprocessing Techniques}

\subsection{Algorithms and Techniques}

Multiple preprocessing techniques were used.\\
First, the dataset has been balanced in regard of the evaluated property, be it being canceled or diverted, so that there are an equal number of uniformly drawn positives and negatives. MIGLIORARE\\
Then the data not already represented as real numbers has been converted; places and carriers, that were alphanumeric codes, had a number assigned based on the code, dates were splitted between the year and the rest, with the latter being hashed MIGLIORARE.\\
The data (now completely composed of real numbers) was then normalized between 0 and 1, to avoid exploding values.\\
Lastly, the data was splitted between the training set (75\%) and the test set (25\%).\\

DIVISO PER MEDIA E VARIANZA??? L2 REGULARIZATION

\subsection{Parallelization}

Keeping in mind that the implementation has to be space and time efficient and scale up to large datasets, the preprocessing part has been carried out using the library PySpark.
PySpark is a wrapper for Python of the library Apache Spark, originally written in Java.\\

The purpose of this library is the handling of parallelized data processing, particularly regarding the Distributed File System Hadoop, also created by the Apache Foundation.
The library handles automatically the work distribution on the available nodes that the system provide, it can be composed of a single machine with multiple cores or a cluster of machines, this improves significantly the scalability of the solution, which can be run on competely different system without code modification.\\

%\begin{figure}
%\includegraphics[width=\linewidth]{spark.ppm}
%\caption{Overview of Spark work distribution}
%\label{fig:spark}
%\end{figure}

The usage of this library created some compatibility issues, because the data structures used by PySpark were not compatible with various parts of the preprocessing section, which had been written initially suing the data science library Pandas.
To solve these problems, it wasn't possible to simply use a conversion and leave the parts written in Pandas as they were, because the computation would have run on a single machine, without parallelization, making the use of PySpark completely pointless.
The issue has been addressed using the PySpark.SQL functionality, which allow to execute queries on a distributed dataframe. For our purposes various UDF(User Defined Functions), have been created, which then have been applied to every column containing certain types of data.\\

The library PySpark is also able to handle the csv file reading and writing, so it has been used to save the preprocessed data to speed up multiple runs on the dataset. To carry out the writing of the various distributed dataframes, various files are created, then at the time of reading, the data is distributed to the various nodes.\\

The preprocessing part of the solution is therefore computed in a distributed manner, using the PySpark dataframe as main data structure to perform the various calculations on the columns of the dataset.
At the end of this section of the solution, the distributed dataframes are merged into one using the collect method. This operation can create memory problems if the dataset is extremely large, but it is not possibile to distribute efficiently the model training
and computing as easily as the preprocessing, so the main part of the solution will be computed using standard Python data structures.

%----------------------------------------------------------------------------------------
%	MODEL
%----------------------------------------------------------------------------------------

\section{Model}

\subsection{Parameters initialization}
Parameters such as Weights and Bias are initialized using a \textbf{uniform distribution} between 0 and 1, with the first one having the same length as the number of columns and the second being a scalar value.

\subsection{Algorithm}

\subsubsection{Normalization}

\subsubsection{Estimate}
The estimate is computed as follows
$$ \hat{y} = \sigma(w^Tx + b) $$
where $\sigma$ is defined as
$$ \sigma(z) = \frac{1}{1 + e^-z} \in (0,1) $$

\subsubsection{Gradient}

$$ \nabla w = \frac{1}{m}x^T(\hat{y} - y) $$
$$\nabla b = \frac{1}{m}\sum(\hat{y} - y) b $$

\subsubsection{Update}


$$ w' = w - \mu \nabla w $$
$$ b' = b - \mu \nabla b $$

\subsubsection{Loss}
For the loss, we used the \textbf{Binary Cross Entropy} function, also called \textbf{Log Loss}.
It is defined as
$$ loss(\hat{y}, y) = -\frac{1}{n}(y log(\hat{y}) + (1-y)log(1-\hat{y}))$$


\subsubsection{Regularization}
Regularization is a technique used to prevent the overfittings. A regularization term is added to the optimization problem (i.e. the gradient calculation) to avoid overfitting.
The used version is called \textbf{L2}, also known as \textbf{Ridge Regression}. MIGLIORARE + BIBLIO\\

The regularization factor for the loss is defined as
$$ \frac{\lambda}{2n}\sum w^2 + b^2 $$ DA VEDERE

The loss becomes
$$ loss(\hat{y}, y) = -\frac{1}{n}(y log(\hat{y}) + (1-y)log(1-\hat{y}))$$

While the weights and bias gradients formula becomes
$$ \nabla w = \frac{1}{m}x^T(\hat{y} - y) + \lambda w $$
and
$$ \nabla b = \frac{1}{m}\sum(\hat{y} - y) + \lambda b$$

DA VEDERE

\subsection{Differences with Sklearn implementation}

In the following chapters the presented model performances will be compared with the Sklearn implementation, that has quite some differences.\\
First, the sklearn version doesn't use the SGD solver, it uses \textbf{L-BFGS-B - Software for Large-scale Bound-constrained Optimization} instead, by default.
% http://users.iems.northwestern.edu/~nocedal/lbfgsb.html
For this reason, the solver doesn't need any form of Learning Rate.\\
Also, with this implementation, the L2 Regularization is enabled by default as well.

% https://medium.com/@aditya97p/l1-and-l2-regularization-237438a9caa6
% https://github.com/mag3141592/LogisticRegression/blob/master/L2RegularizedLogisticRegression.py

%----------------------------------------------------------------------------------------
%	PERFORMANCES 
%----------------------------------------------------------------------------------------

\section{Performances}

- come scalano le operazioni di numpy effettuate

%----------------------------------------------------------------------------------------
%	EXPERIMENTS
%----------------------------------------------------------------------------------------

\section{Experiments}

\subsection{Canceled Flights}

\subsubsection{Changing the Learning Rate}

\begin{center}
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Iterations & LR & L2 & Loss \\
		\hline
		100        & 0  & 0  & 0    \\
		500        & 0  & 0  & 0    \\
		1000       & 0  & 0  & 0    \\
		\hline
	\end{tabular}
	\captionof{table}{Your caption here}
\end{center}

\subsubsection{Adding the L2 regularization}

\begin{center}
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Iterations & LR & L2 & Loss \\
		\hline
		100        & 0  & 0  & 0    \\
		500        & 0  & 0  & 0    \\
		1000       & 0  & 0  & 0    \\
		\hline
	\end{tabular}
	\captionof{table}{Your caption here}
\end{center}

\subsubsection{Comparison with Sklearn}

\begin{center}
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Iterations & LR & L2 & Loss \\
		\hline
		100        & 0  & 0  & 0    \\
		500        & 0  & 0  & 0    \\
		1000       & 0  & 0  & 0    \\
		\hline
	\end{tabular}
	\captionof{table}{Your caption here}
\end{center}

\subsection{Diverted Flights}

\subsubsection{Changing the Learning Rate}

\begin{center}
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Iterations & LR & L2 & Loss \\
		\hline
		100        & 0  & 0  & 0    \\
		500        & 0  & 0  & 0    \\
		1000       & 0  & 0  & 0    \\
		\hline
	\end{tabular}
	\captionof{table}{Your caption here}
\end{center}

\subsubsection{Adding the L2 regularization}

\begin{center}
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Iterations & LR & L2 & Loss \\
		\hline
		100        & 0  & 0  & 0    \\
		500        & 0  & 0  & 0    \\
		1000       & 0  & 0  & 0    \\
		\hline
	\end{tabular}
	\captionof{table}{Your caption here}
\end{center}

\subsubsection{Comparison with Sklearn}

\begin{center}
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Iterations & LR & L2 & Loss \\
		\hline
		100        & 0  & 0  & 0    \\
		500        & 0  & 0  & 0    \\
		1000       & 0  & 0  & 0    \\
		\hline
	\end{tabular}
	\captionof{table}{Your caption here}
\end{center}

- nostro modello senza LR
- nostro modello con LR v1
- nostro modello con LR v2
- nostro modello con LR v3
- nostro modello con LR migliore e L2
- confronto modello di sklearn con i valori migliori


%----------------------------------------------------------------------------------------
%	RESULTS AND CONCLUSIONS
%----------------------------------------------------------------------------------------

\section{Results and Conclusions}

\subsection{Space and Time}


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography % Output the bibliography

%----------------------------------------------------------------------------------------

\end{document}