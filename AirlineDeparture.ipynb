{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from kaggle.api.kaggle_api_extended import KaggleApi\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import monotonically_increasing_id, col, udf, rand\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "import pyspark.sql as ps\n",
                "from zlib import crc32\n",
                "import time as tm\n",
                "from datetime import datetime as dt\n",
                "import itertools\n",
                "from dataclasses import dataclass\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.rdd import RDD\n",
                "from pyspark.broadcast import Broadcast\n",
                "import findspark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = './data'\n",
                "worker_nodes = \"*\"\n",
                "problem_to_solve = 'CANCELLED'\n",
                "\n",
                "dataset_limit = 10000\n",
                "use_all_dataset_frames = True\n",
                "fold_number = 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DA SCRIVERE\n",
                "- perche' usiamo i dataframe invece degli rdd nella prima parte\n",
                "- aggiungere k fold cross validation\n",
                "- aggiungere griglia parametri\n",
                "- aggiungere label stratification\n",
                "- aggiungere performance modello pyspark\n",
                "- aggiungere check e info extra su dataset di base (es sbilanciamento)\n",
                "- auroc, auprc, f1, \n",
                "- confronto con tree classifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['KAGGLE_USERNAME'] = \"davidetricella\"\n",
                "os.environ['KAGGLE_KEY'] = \"e1ab3aae4a07f36b37a3a8bace74d9df\"\n",
                "\n",
                "\n",
                "dataset = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
                "path = './data'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_dataset():\n",
                "    if not os.path.isdir(path):\n",
                "        os.mkdir(path)\n",
                "    if not os.listdir(path):\n",
                "        try:\n",
                "            api = KaggleApi()\n",
                "            api.authenticate()\n",
                "            api.dataset_download_files(dataset, path, unzip=True, quiet=False)\n",
                "        except:\n",
                "            print(\"Error downloading the dataset\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataframe_schema = StructType([\n",
                "    StructField('FL_DATE', StringType(), True),\n",
                "    StructField('OP_CARRIER', StringType(), True),\n",
                "    StructField('ORIGIN', StringType(), True),\n",
                "    StructField('DEST', StringType(), True),\n",
                "    StructField('CRS_DEP_TIME', StringType(), True),\n",
                "    StructField('CRS_ARR_TIME', StringType(), True),\n",
                "    StructField('CANCELLED', StringType(), True),\n",
                "    StructField('DIVERTED', StringType(), True),\n",
                "    StructField('CRS_ELAPSED_TIME', StringType(), True),\n",
                "    StructField('DISTANCE', StringType(), True)\n",
                "])\n",
                "\n",
                "columns_to_get = [\n",
                "    'FL_DATE',\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "findspark.init()\n",
                "findspark.find()\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                ".appName(\"Airline Departure\") \\\n",
                ".master('local[' + worker_nodes + ']') \\\n",
                ".getOrCreate()\n",
                "\n",
                "context = spark.sparkContext"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset():\n",
                "    data = spark.read.format(\"csv\") \\\n",
                "        .option(\"header\", True) \\\n",
                "        .load(path + '/preprocessed')\n",
                "\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def save_dataset(data):\n",
                "    data.write.format('csv').option('header', True).mode('overwrite').option(\n",
                "        'sep', ',').save(path + '/preprocessed')\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def check_preprocessed_data_exists() -> bool:\n",
                "    files = os.listdir('./data')\n",
                "    for f in files:\n",
                "        if f.startswith('preprocessed'):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def get_dataset(limit: float = -1, allFrames: bool = True):\n",
                "    files = os.listdir(path)\n",
                "    big_frame = spark.createDataFrame(\n",
                "        spark.sparkContext.emptyRDD(), schema=dataframe_schema)\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            frame = spark.read.option(\"header\", True).csv(path + '/' + f)\n",
                "            frame = frame.select(columns_to_get)\n",
                "            frame = frame.orderBy(rand())\n",
                "\n",
                "            if limit != -1:\n",
                "                frame = frame.limit(limit)\n",
                "\n",
                "            big_frame = frame.union(big_frame)\n",
                "\n",
                "    big_frame = big_frame.select(\n",
                "        \"*\").withColumn(\"index\", monotonically_increasing_id())\n",
                "    big_frame.count()\n",
                "\n",
                "    return big_frame\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "columns_to_remove_for_canceled = [\n",
                "    'DIVERTED',  # the flight has been diverted to an unplanned airport\n",
                "]\n",
                "\n",
                "columns_to_remove_for_diverted = [\n",
                "    'CANCELLED',  # the flight has been cancelled\n",
                "]\n",
                "\n",
                "preprocess_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'FL_DATE',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'index'\n",
                "]\n",
                "\n",
                "max_distance = 4970"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "#CHARTS PLOTTING\n",
                "def plot_balancing_chart(data: ps.DataFrame, label: str):\n",
                "  total_positives = data.filter(col(label) == 1).count()\n",
                "  total_negatives = data.filter(col(label) == 0).count()\n",
                "  fig, ax = plt.subplots()\n",
                "\n",
                "  labels = ['REGULAR', label]\n",
                "  counts = [total_negatives, total_positives]\n",
                "  bar_colors = ['tab:blue', 'tab:red']\n",
                "\n",
                "  ax.bar(labels, counts, color=bar_colors)\n",
                "\n",
                "  ax.set_ylabel('Counts')\n",
                "  ax.set_title('Regular flights and problematic flights counts')\n",
                "\n",
                "  plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "#PREPROCESSING EXECUTION\n",
                "def print_and_save_time(s: str):\n",
                "  time_file.write(s + '\\n')\n",
                "  print(s)\n",
                "time_file = open(\"./data/times.txt\", \"w\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 43:>                                                       (0 + 16) / 16]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataframe rows: 110000\n",
                        "Dataset reading concluded: 25.13623833656311 seconds\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "#DATASET READING\n",
                "download_dataset()\n",
                "start_time = tm.time()\n",
                "data = get_dataset(dataset_limit, use_all_dataset_frames)\n",
                "print(\"Dataframe rows: \" + str(data.count()))\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 66:=======>                                                (2 + 14) / 16]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataframe rows after NaN dropping: 110000\n",
                        "Null values removal concluded: 26.10139751434326 seconds\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "# Nan ROWS DROPPING\n",
                "common_start_time = tm.time()\n",
                "\n",
                "data = data.dropna(how='any')\n",
                "print(\"Dataframe rows after NaN dropping: \" + str(data.count()))\n",
                "\n",
                "null_removal_finish_time = tm.time() - common_start_time\n",
                "print_and_save_time(\"Null values removal concluded: \" +\n",
                "                    str(null_removal_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 134:===>                                                   (1 + 15) / 16]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Balanced dataframe rows: 13164\n",
                        "Dataset balancing concluded: 75.5889539718628 seconds\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "#DATAFRAME BALANCING\n",
                "start_time = tm.time()\n",
                "irregular_flights = data.filter(col(problem_to_solve) == 1)\n",
                "\n",
                "regular_flights = data.filter(col(problem_to_solve) == 0)\n",
                "\n",
                "data = regular_flights.limit(irregular_flights.count()).\\\n",
                "    union(irregular_flights).\\\n",
                "    orderBy(rand())\n",
                "print(\"Balanced dataframe rows: \" + str(data.count()))\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset balancing concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 190:==================================>                    (10 + 6) / 16]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Columns conversion concluded: 48.093705892562866 seconds\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "#COLUMN CONVERSIONS\n",
                "columns_start_time = tm.time()\n",
                "\n",
                "@udf(returnType=DoubleType())\n",
                "def str_to_float(s: str):\n",
                "  encoding = \"utf-8\"\n",
                "  b = s.encode(encoding)\n",
                "  return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "date_multiplier: float = 1 / 365\n",
                "@udf(returnType=DoubleType())\n",
                "def date_to_day_of_year(date_string) -> float:\n",
                "  date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "  day = date.timetuple().tm_yday - 1\n",
                "  return day * date_multiplier\n",
                "\n",
                "@udf(returnType=DoubleType())\n",
                "def time_to_interval(time) -> float:\n",
                "  t = int(float(time))\n",
                "  h = t // 100\n",
                "  m = t % 100\n",
                "  t = h * 60 + m\n",
                "  return float(t / 1140)\n",
                "\n",
                "distance_multiplier = float(1) / float(max_distance)\n",
                "\n",
                "data = data.select(\n",
                "  (data.CANCELLED.cast('double')).alias(\"CANCELLED\"),\n",
                "  (data.DIVERTED.cast('double')).alias(\"DIVERTED\"),\n",
                "  str_to_float(data.OP_CARRIER).alias(\"OP_CARRIER\"),\n",
                "  str_to_float(data.ORIGIN).alias(\"ORIGIN\"),\n",
                "  str_to_float(data.DEST).alias(\"DEST\"),\n",
                "  date_to_day_of_year(data.FL_DATE).alias(\"FL_DATE\"),\n",
                "  time_to_interval(data.CRS_DEP_TIME).alias(\"CRS_DEP_TIME\"),\n",
                "  time_to_interval(data.CRS_ARR_TIME).alias(\"CRS_ARR_TIME\"),\n",
                "  time_to_interval(data.CRS_ELAPSED_TIME).alias(\"CRS_ELAPSED_TIME\"),\n",
                "  (data.DISTANCE.cast('double') * distance_multiplier).alias(\"DISTANCE\")\n",
                ")\n",
                "data.count()\n",
                "\n",
                "columns_finish_time = tm.time() - columns_start_time\n",
                "print_and_save_time(\"Columns conversion concluded: \" +\n",
                "                    str(columns_finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- CANCELLED: double (nullable = true)\n",
                        " |-- DIVERTED: double (nullable = true)\n",
                        " |-- OP_CARRIER: double (nullable = true)\n",
                        " |-- ORIGIN: double (nullable = true)\n",
                        " |-- DEST: double (nullable = true)\n",
                        " |-- FL_DATE: double (nullable = true)\n",
                        " |-- CRS_DEP_TIME: double (nullable = true)\n",
                        " |-- CRS_ARR_TIME: double (nullable = true)\n",
                        " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
                        " |-- DISTANCE: double (nullable = true)\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "22/11/26 18:36:31 ERROR Executor: Exception in task 0.0 in stage 314.0 (TID 1744)\n",
                        "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
                        "  File \"/tmp/ipykernel_4634/1496745023.py\", line 13, in date_to_day_of_year\n",
                        "  File \"/usr/lib/python3.9/_strptime.py\", line 568, in _strptime_datetime\n",
                        "    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n",
                        "  File \"/usr/lib/python3.9/_strptime.py\", line 349, in _strptime\n",
                        "    raise ValueError(\"time data %r does not match format %r\" %\n",
                        "ValueError: time data '0.5917808219178082' does not match format '%Y-%m-%d'\n",
                        "\n",
                        "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
                        "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
                        "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
                        "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
                        "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
                        "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
                        "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
                        "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage30.processNext(Unknown Source)\n",
                        "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
                        "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
                        "22/11/26 18:36:31 WARN TaskSetManager: Lost task 0.0 in stage 314.0 (TID 1744) (172.19.80.137 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
                        "  File \"/tmp/ipykernel_4634/1496745023.py\", line 13, in date_to_day_of_year\n",
                        "  File \"/usr/lib/python3.9/_strptime.py\", line 568, in _strptime_datetime\n",
                        "    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n",
                        "  File \"/usr/lib/python3.9/_strptime.py\", line 349, in _strptime\n",
                        "    raise ValueError(\"time data %r does not match format %r\" %\n",
                        "ValueError: time data '0.5917808219178082' does not match format '%Y-%m-%d'\n",
                        "\n",
                        "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
                        "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
                        "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
                        "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
                        "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
                        "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
                        "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
                        "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
                        "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage30.processNext(Unknown Source)\n",
                        "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
                        "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
                        "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
                        "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
                        "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
                        "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
                        "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
                        "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
                        "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
                        "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
                        "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
                        "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
                        "\n",
                        "22/11/26 18:36:31 ERROR TaskSetManager: Task 0 in stage 314.0 failed 1 times; aborting job\n"
                    ]
                },
                {
                    "ename": "PythonException",
                    "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_4634/1496745023.py\", line 13, in date_to_day_of_year\n  File \"/usr/lib/python3.9/_strptime.py\", line 568, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"/usr/lib/python3.9/_strptime.py\", line 349, in _strptime\n    raise ValueError(\"time data %r does not match format %r\" %\nValueError: time data '0.5917808219178082' does not match format '%Y-%m-%d'\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data\u001b[39m.\u001b[39mprintSchema()\n\u001b[0;32m----> 2\u001b[0m data\u001b[39m.\u001b[39;49mshow()\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
                        "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_4634/1496745023.py\", line 13, in date_to_day_of_year\n  File \"/usr/lib/python3.9/_strptime.py\", line 568, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"/usr/lib/python3.9/_strptime.py\", line 349, in _strptime\n    raise ValueError(\"time data %r does not match format %r\" %\nValueError: time data '0.5917808219178082' does not match format '%Y-%m-%d'\n"
                    ]
                }
            ],
            "source": [
                "data.printSchema()\n",
                "data.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o423.save.\n: ExitCodeException exitCode=1: chmod: changing permissions of '/mnt/c/Users/manue/Home/University/AirlineDeparture/data/preprocessed': Operation not permitted\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:209)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_dataset(data)\n",
                        "Cell \u001b[0;32mIn [14], line 10\u001b[0m, in \u001b[0;36msave_dataset\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_dataset\u001b[39m(data):\n\u001b[0;32m---> 10\u001b[0m     data\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moption(\n\u001b[1;32m     11\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39msep\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/preprocessed\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPreprocessed dataset saved\u001b[39m\u001b[39m'\u001b[39m)\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
                        "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o423.save.\n: ExitCodeException exitCode=1: chmod: changing permissions of '/mnt/c/Users/manue/Home/University/AirlineDeparture/data/preprocessed': Operation not permitted\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:209)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
                    ]
                }
            ],
            "source": [
                "save_dataset(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data.printScheme()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_balancing_chart(data, problem_to_solve)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Z Score Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "z_start_time = tm.time()\n",
                "column_list = data.columns\n",
                "column_mean_dict = dict()\n",
                "column_stddv_dict = dict()\n",
                "\n",
                "for c in column_list:\n",
                "    column_mean_dict[c] = data.agg({c: 'mean'}).head()[0]\n",
                "    column_stddv_dict[c] = data.agg({c: 'stddev'}).head()[0]\n",
                "\n",
                "data = data.select(\n",
                "  problem_to_solve,\n",
                "\n",
                "  ((data.OP_CARRIER - column_mean_dict[\"OP_CARRIER\"]) / column_stddv_dict[\"OP_CARRIER\"]).alias('OP_CARRIER'),\n",
                "\n",
                "  ((data.ORIGIN - column_mean_dict[\"ORIGIN\"]) / column_stddv_dict[\"ORIGIN\"]).alias('ORIGIN'),\n",
                "\n",
                "  ((data.DEST - column_mean_dict[\"DEST\"]) / column_stddv_dict[\"DEST\"]).alias('DEST'),\n",
                "\n",
                "  ((data.FL_DATE - column_mean_dict[\"FL_DATE\"]) / column_stddv_dict[\"FL_DATE\"]).alias('FL_DATE'),\n",
                "\n",
                "  ((data.CRS_DEP_TIME - column_mean_dict[\"CRS_DEP_TIME\"]) / column_stddv_dict[\"CRS_DEP_TIME\"]).alias('CRS_DEP_TIME'),\n",
                "\n",
                "  ((data.CRS_ARR_TIME - column_mean_dict[\"CRS_ARR_TIME\"]) /  column_stddv_dict[\"CRS_ARR_TIME\"]).alias('CRS_ARR_TIME'),\n",
                "\n",
                "  ((data.CRS_ELAPSED_TIME - column_mean_dict[\"CRS_ELAPSED_TIME\"]) / column_stddv_dict[\"CRS_ELAPSED_TIME\"]).alias('CRS_ELAPSED_TIME'),\n",
                "\n",
                "  ((data.DISTANCE - column_mean_dict[\"DISTANCE\"]) / column_stddv_dict[\"DISTANCE\"]).alias('DISTANCE'),\n",
                ")\n",
                "data.count()\n",
                "\n",
                "z_finish_time = tm.time() - z_start_time\n",
                "print_and_save_time(\"Z score normalization concluded: \" +\n",
                "                    str(z_finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "ename": "PythonException",
                    "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\manue\\AppData\\Local\\Temp\\ipykernel_23328\\1496745023.py\", line 13, in date_to_day_of_year\n  File \"c:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\_strptime.py\", line 568, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"c:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\_strptime.py\", line 349, in _strptime\n    raise ValueError(\"time data %r does not match format %r\" %\nValueError: time data '0.873972602739726' does not match format '%Y-%m-%d'\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [18], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m split_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39m#data = data.dropDuplicates()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m#print(\"Dataframe rows after duplicates dropping: \" + str(data.count()))\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m k_elements_half_number \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mfloor((data\u001b[39m.\u001b[39;49mcount() \u001b[39m/\u001b[39m fold_number) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch elements: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(k_elements_half_number))\n\u001b[0;32m     11\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    795\u001b[0m     \u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \n\u001b[0;32m    797\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[39m    2\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcount())\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
                        "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\manue\\AppData\\Local\\Temp\\ipykernel_23328\\1496745023.py\", line 13, in date_to_day_of_year\n  File \"c:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\_strptime.py\", line 568, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"c:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\_strptime.py\", line 349, in _strptime\n    raise ValueError(\"time data %r does not match format %r\" %\nValueError: time data '0.873972602739726' does not match format '%Y-%m-%d'\n"
                    ]
                }
            ],
            "source": [
                "#DATA SPLITTING\n",
                "start_time = tm.time()\n",
                "split_list = []\n",
                "\n",
                "#data = data.dropDuplicates()\n",
                "#print(\"Dataframe rows after duplicates dropping: \" + str(data.count()))\n",
                "\n",
                "k_elements_half_number = math.floor((data.count() / fold_number) / 2)\n",
                "print(\"Batch elements: \" + str(k_elements_half_number))\n",
                "\n",
                "i = 0\n",
                "while i < fold_number:\n",
                "    k_positive_sample = data.where(\n",
                "        col(problem_to_solve) == 1).limit(k_elements_half_number)\n",
                "\n",
                "    k_negative_sample = data.where(\n",
                "        col(problem_to_solve) == 0).limit(k_elements_half_number)\n",
                "\n",
                "    k_sample = k_positive_sample.union(k_negative_sample)\n",
                "    print(\"Total k sample rows: \" + str(k_sample.count()))\n",
                "\n",
                "    split_list.append(k_sample)\n",
                "    data = data.subtract(k_sample)\n",
                "\n",
                "    print(\"Split \" + str(i + 1) + \" of \" + str(fold_number) + \" completed\")\n",
                "    print(\"Dataframe rows: \" + str(data.count()))\n",
                "    i += 1\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bonus: Pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pandas_load_dataset():\n",
                "    \n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def pandas_save_dataset(data):\n",
                "    data.to_csv(path_or_buf=path + '/' + 'preprocessed.csv', index=False)\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "# Data Load\n",
                "\n",
                "files = os.listdir(path)\n",
                "data = pd.DataFrame()\n",
                "\n",
                "for f in files:\n",
                "    if f.endswith('.csv'):\n",
                "        frame = pd.read_csv(filepath_or_buffer=path +\n",
                "                            '/' + f, usecols=columns_to_get)\n",
                "        data = pd.concat([data, frame])\n",
                "\n",
                "data = data.dropna(how='any', axis='index')\n",
                "        \n",
                "# Problem Selection\n",
                "\n",
                "irregulars = data.filter([problem_to_solve], like=1)\n",
                "regulars = data.filter([problem_to_solve], like=0)\n",
                "\n",
                "data = pd.concat([regulars.sample(len(irregulars)), irregulars]).sample(frac=1)\n",
                "\n",
                "oppositeIndex = 'DIVERTED' if problem_to_solve == 'CANCELLED' else 'CANCELLED'\n",
                "data = data.drop(oppositeIndex, axis=1)\n",
                "\n",
                "# Names Conversion\n",
                "\n",
                "def str_to_float(s: str):\n",
                "    encoding = \"utf-8\"\n",
                "    b = s.encode(encoding)\n",
                "    return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "for c in ['OP_CARRIER', 'ORIGIN', 'DEST']:\n",
                "    data[c] = data[c].apply(str_to_float)\n",
                "\n",
                "# Dates Conversion\n",
                "\n",
                "multiplier: float = 1 / 365\n",
                "\n",
                "def date_to_day_of_year(date_string) -> float:\n",
                "    date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "    day = date.timetuple().tm_yday - 1\n",
                "    return day * multiplier\n",
                "\n",
                "for i in date_columns_to_convert:\n",
                "    data[i] = data[i].apply(date_to_day_of_year)\n",
                "\n",
                "# Time Conversion\n",
                "    \n",
                "def time_to_interval(time) -> float:\n",
                "    t = int(float(time))\n",
                "    h = t // 100\n",
                "    m = t % 100\n",
                "    t = h * 60 + m\n",
                "    return float(t / 1140)\n",
                "\n",
                "for c in time_columns_to_convert:\n",
                "    data[c] = data[c].apply(time_to_interval)\n",
                "\n",
                "# Distance Conversion\n",
                "    \n",
                "multiplier: float = float(1) / float(max_distance)\n",
                "\n",
                "for c in numeric_columns_to_convert:\n",
                "    data[c] = data[c].apply(lambda x: x * multiplier)\n",
                "\n",
                "# Create Folds\n",
                "\n",
                "folds = []\n",
                "\n",
                "data.drop_duplicates(inplace=True)\n",
                "\n",
                "irregulars = data.filter([problem_to_solve], like=1)\n",
                "regulars = data.filter([problem_to_solve], like=0)\n",
                "\n",
                "k_elements_half_number = round((len(data) / fold_number) / 2)\n",
                "\n",
                "for i in range(1, fold_number + 1):\n",
                "    k_irregulars_sample = irregulars.head(k_elements_half_number)\n",
                "    k_regulars_sample = regulars.head(k_elements_half_number)\n",
                "    k_sample = pd.concat([k_irregulars_sample, k_regulars_sample])\n",
                "\n",
                "    folds.append(k_sample.to_numpy())\n",
                "    irregulars = irregulars.drop(k_irregulars_sample.index)\n",
                "    regulars = regulars.drop(k_regulars_sample.index)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generic Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    '''\n",
                "    Calculates the sigmoid of the given data\n",
                "    '''\n",
                "    g = 1.0 / (1.0 + np.exp(-x))\n",
                "    return g\n",
                "\n",
                "def binary_cross_entropy(y, y_label, w, l2):\n",
                "    '''\n",
                "    Calculates the binary cross entropy loss of the calculated y and the given y_label\n",
                "    '''\n",
                "    loss = -np.mean(y_label*(np.log(y)) + (1-y_label)\n",
                "                    * np.log(1-y)) + regularize(w, l2)\n",
                "    return loss\n",
                "\n",
                "def regularize(W, l2):\n",
                "    '''\n",
                "    Calculates the regularization term for the loss\n",
                "    '''\n",
                "    return (l2 / 2) * np.sum(np.square(W))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parallel Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ParallelLogisticRegression:\n",
                "    iterations: int\n",
                "    learning_rate: float\n",
                "    batch_size: int\n",
                "    l2: float\n",
                "    W: Broadcast\n",
                "    b: float\n",
                "\n",
                "def initialize(self: ParallelLogisticRegression, size):\n",
                "    self.W = context.broadcast(np.random.rand(size))\n",
                "    self.b = np.random.rand()\n",
                "\n",
                "\n",
                "def parallel_train(self: ParallelLogisticRegression, data: ps.DataFrame):\n",
                "    initialize(self, len(data.columns) - 1)\n",
                "\n",
                "    num_chunks = X.count() // self.batch_size\n",
                "    chunk_percent = 1/num_chunks\n",
                "\n",
                "    batches = data.randomSplit([chunk_percent] * num_chunks)\n",
                "\n",
                "    Y_labels = [b.select(problem_to_solve).rdd for b in batches]\n",
                "    X = [b.drop(problem_to_solve).rdd for b in batches]\n",
                "\n",
                "    losses = []\n",
                "    gradients = []\n",
                "\n",
                "    for _ in range(self.iterations):\n",
                "        _losses = []\n",
                "        _gradients = [] \n",
                "\n",
                "        for b_X, b_Y_labels in zip(X, Y_labels):\n",
                "            Y = self.evaluate(b_X)\n",
                "            _losses.append(binary_cross_entropy(\n",
                "                Y, b_Y_labels, self.W, self.l2))\n",
                "            (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "            _gradients.append(dW)\n",
                "            self.update(dW, db)\n",
                "        losses.append(np.mean(_losses))\n",
                "        gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)\n",
                "\n",
                "def parallel_evaluate(self: ParallelLogisticRegression, X: RDD):\n",
                "    Z = X.map(lambda x: np.dot(x, self.W.value)).reduce(lambda a, b: a+b + self.b)\n",
                "    Z = Z.map(lambda x: sigmoid(x))\n",
                "    return Z\n",
                "        \n",
                "def update(self, dW: list[float], db: float):\n",
                "        self.W = context.broadcast(self.W.value - self.learning_rate * dW)\n",
                "        self.b = self.b - self.learning_rate * db"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Serial Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SerialLogisticRegression():\n",
                "    def __init__(self, iterations: int, learning_rate: float, batch_size: int, l2: float):\n",
                "        self.iterations = iterations\n",
                "        self.learning_rate = learning_rate\n",
                "        self.batch_size = batch_size\n",
                "        self.l2 = l2\n",
                "\n",
                "    def initialize(self, columns_number):\n",
                "        self.W = np.random.rand(columns_number)\n",
                "        self.b = np.random.rand()\n",
                "\n",
                "    def evaluate(self, X):\n",
                "        Z = np.dot(X, self.W) + self.b\n",
                "        Z = sigmoid(Z)\n",
                "        return Z\n",
                "\n",
                "    def gradient(self, X, Y, Y_label):\n",
                "        '''\n",
                "        Calculates the gradient w.r.t weights and bias\n",
                "        '''\n",
                "\n",
                "        # Number of training examples.\n",
                "        m = X.shape[0]\n",
                "\n",
                "        # Gradient of loss w.r.t weights with regularization\n",
                "        dw = (1/m)*np.dot(X.T, (Y - Y_label)) + self.l2 * self.W\n",
                "\n",
                "        # Gradient of loss w.r.t bias with regularization\n",
                "        db = (1/m)*np.sum((Y - Y_label))\n",
                "\n",
                "        return dw, db\n",
                "\n",
                "    def update(self, dW, db):\n",
                "        self.W = self.W - self.learning_rate * dW\n",
                "        self.b = self.b - self.learning_rate * db\n",
                "\n",
                "    def train(self, X, Y_labels, iterations = 10):\n",
                "        self.initialize(X.shape[1])\n",
                "        losses = []\n",
                "        gradients = []\n",
                "\n",
                "        for _ in range(iterations):\n",
                "            _losses = []\n",
                "            _gradients = []\n",
                "            for b in range(X.shape[0]//self.batch_size):\n",
                "                b_X = X[b*self.batch_size:b*self.batch_size+self.batch_size, :]\n",
                "                b_Y_labels = Y_labels[b*self.batch_size:b *\n",
                "                                      self.batch_size+self.batch_size]\n",
                "                Y = self.evaluate(b_X)\n",
                "                _losses.append(binary_cross_entropy(\n",
                "                    Y, b_Y_labels, self.W, self.l2))\n",
                "                (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "                _gradients.append(dW)\n",
                "                self.update(dW, db)\n",
                "            losses.append(np.mean(_losses))\n",
                "            gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiments\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_roc(labels, results, name):\n",
                "    labels_and_results = sorted(\n",
                "        list(zip(labels, map(lambda x: x, results))), key=lambda x: x[1])\n",
                "\n",
                "    labels_by_weights = np.array([k for (k, _) in labels_and_results])\n",
                "\n",
                "    length = labels_by_weights.size\n",
                "\n",
                "    true_positives = labels_by_weights.cumsum()\n",
                "\n",
                "    num_positive = true_positives[-1]\n",
                "\n",
                "    false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
                "\n",
                "    true_positives_rate = true_positives / num_positive\n",
                "    false_positives_rate = false_positives / (length - num_positive)\n",
                "\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
                "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
                "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
                "    plt.plot(false_positives_rate, true_positives_rate,\n",
                "             color='#8cbfd0', linestyle='-', linewidth=3.)\n",
                "    plt.plot((0., 1.), (0., 1.), linestyle='--',\n",
                "             color='#d6ebf2', linewidth=2.)\n",
                "\n",
                "    plt.savefig('./data/{}_roc.png'.format(name))\n",
                "    fig.clear()\n",
                "    plt.close()\n",
                "\n",
                "def plot_loss_gradient(iterations, train_losses, gradients, name):\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlabel('Iterations')\n",
                "    ax.set_ylabel('Loss/Gradient')\n",
                "    ax.set_title(name)\n",
                "    ax.plot(range(iterations), train_losses, label='Loss')\n",
                "    ax.plot(range(iterations), gradients, label='Gradient')\n",
                "    ax.grid()\n",
                "    ax.legend()\n",
                "\n",
                "    fig.savefig(\"./data/{}.png\".format(name))\n",
                "    fig.clear()\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparamters Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid = { 'iter': [100, 200, 500], 'lr': [0.001, 0.01, 0.1], 'l2': [0, 0.1, 0.001], 'batch_size': [0, 20]}\n",
                "\n",
                "params = list(itertools.product(*grid.values()))\n",
                "\n",
                "params"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### K-Fold Cross Validation\n",
                "The following code defines a base class with train and evaluation methods to apply the K-Fold Cross Validation to each model\n",
                "\n",
                "DIVIDIAMOLA IN 10, 8 + 1 NEL TRAINING E 1 PER EVALUATION FINALE?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "class Evaluator:\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        self.iterations = iterations\n",
                "        self.lr = lr\n",
                "        self.l2 = l2\n",
                "        self.batch_size = batch_size\n",
                "\n",
                "    def train(self, data):\n",
                "        total_train_losses = []\n",
                "        total_test_losses = []\n",
                "        for i, fold in enumerate(data):\n",
                "            train_data = data[:i] + data[i + 1:]\n",
                "            test_data = fold\n",
                "            losses,  = self.train_impl(train_data)\n",
                "            total_train_losses.append(losses) \n",
                "            loss = self.test_impl(test_data)\n",
                "            total_test_losses.append(loss)\n",
                "        return total_train_losses, total_test_losses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ParallelModelEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = ParallelLogisticRegression(self.iterations, self.lr, self.batch_size)\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        return parallel_train(self.model, train_data)\n",
                "    \n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        value: RDD = parallel_evaluate(self.model, test_data)\n",
                "        return binary_cross_entropy(np.array(value.collect()), y_label, self.model.W.value, self.l2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SequentialEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = SerialLogisticRegression(self.iterations, self.lr, self.batch_size)\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        pass\n",
                "\n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
                "from pyspark.mllib.regression import LabeledPoint\n",
                "\n",
                "class MLibModelEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = None\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        labels = train_data.rdd.map(lambda x: LabeledPoint(label = x[0], features=x[1:]))        \n",
                "        self.model = LogisticRegressionWithSGD.train(labels, iterations=self.iterations, regParam=self.lr, convergenceTol=0, validateData=False, intercept=True)\n",
                "        return [], []\n",
                "\n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        return test_data.rdd.map(lambda x: float(self.model.predict(x))).reduce(lambda x, y: x + y) / test_data.rdd.count()\n",
                "        "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.2 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
