{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from kaggle.api.kaggle_api_extended import KaggleApi\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import monotonically_increasing_id, col, udf, rand\n",
                "import pyspark.sql as ps\n",
                "from pyspark.broadcast import Broadcast\n",
                "import matplotlib.pyplot as plt\n",
                "from zlib import crc32\n",
                "import time as tm\n",
                "from datetime import datetime as dt\n",
                "import itertools\n",
                "from dataclasses import dataclass\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.rdd import RDD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = './data'\n",
                "worker_nodes = \"*\"\n",
                "problem_to_solve = 'CANCELLED'\n",
                "\n",
                "dataset_limit = 1000\n",
                "use_all_dataset_frames = True\n",
                "number_of_splits = 10\n",
                "\n",
                "dataset = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
                "path = './data'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DA SCRIVERE\n",
                "- perche' usiamo i dataframe invece degli rdd nella prima parte\n",
                "- aggiungere k fold cross validation\n",
                "- aggiungere griglia parametri\n",
                "- aggiungere label stratification\n",
                "- aggiungere performance modello pyspark\n",
                "- aggiungere check e info extra su dataset di base (es sbilanciamento)\n",
                "- auroc, auprc, f1, \n",
                "- confronto con tree classifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['KAGGLE_USERNAME'] = \"davidetricella\"\n",
                "os.environ['KAGGLE_KEY'] = \"e1ab3aae4a07f36b37a3a8bace74d9df\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_dataset():\n",
                "    if not os.path.isdir(path):\n",
                "        os.mkdir(path)\n",
                "    if not os.listdir(path):\n",
                "        try:\n",
                "            api = KaggleApi()\n",
                "            api.authenticate()\n",
                "            api.dataset_download_files(dataset, path, unzip=True, quiet=False)\n",
                "        except:\n",
                "            print(\"Error downloading the dataset\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataframe_schema = StructType([\n",
                "    StructField('FL_DATE', StringType(), True),\n",
                "    StructField('OP_CARRIER', StringType(), True),\n",
                "    StructField('ORIGIN', StringType(), True),\n",
                "    StructField('DEST', StringType(), True),\n",
                "    StructField('CRS_DEP_TIME', StringType(), True),\n",
                "    StructField('CRS_ARR_TIME', StringType(), True),\n",
                "    StructField('CANCELLED', StringType(), True),\n",
                "    StructField('DIVERTED', StringType(), True),\n",
                "    StructField('CRS_ELAPSED_TIME', StringType(), True),\n",
                "    StructField('DISTANCE', StringType(), True)\n",
                "])\n",
                "\n",
                "columns_to_get = [\n",
                "    'FL_DATE',\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                ".appName(\"Airline Departure\") \\\n",
                ".master('local[' + worker_nodes + ']') \\\n",
                ".getOrCreate()\n",
                "\n",
                "context = spark.sparkContext"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset():\n",
                "    data = spark.read.format(\"csv\") \\\n",
                "        .option(\"header\", True) \\\n",
                "        .load(path + '/preprocessed')\n",
                "\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def save_dataset(data):\n",
                "    data.write.format('csv').option('header', True).mode('overwrite').option(\n",
                "        'sep', ',').save(path + '/preprocessed')\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def check_preprocessed_data_exists() -> bool:\n",
                "    files = os.listdir('./data')\n",
                "    for f in files:\n",
                "        if f.startswith('preprocessed'):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def get_dataset(limit: float = -1, allFrames: bool = True):\n",
                "    files = os.listdir(path)\n",
                "    big_frame = spark.createDataFrame(\n",
                "        spark.sparkContext.emptyRDD(), schema=dataframe_schema)\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            frame = spark.read.option(\"header\", True).csv(path + '/' + f)\n",
                "            frame = frame.select(columns_to_get)\n",
                "            frame = frame.sample(fraction=1.0, withReplacement=False)\n",
                "\n",
                "            if limit != -1:\n",
                "                frame = frame.limit(limit)\n",
                "\n",
                "            big_frame = frame.union(big_frame)\n",
                "\n",
                "    big_frame = big_frame.select(\n",
                "        \"*\").withColumn(\"index\", monotonically_increasing_id())\n",
                "    big_frame.count()\n",
                "\n",
                "    return big_frame\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "default_values = {\n",
                "    'CANCELLED': 0,\n",
                "    'DIVERTED': 0\n",
                "}\n",
                "\n",
                "columns_to_remove_for_canceled = [\n",
                "    'DIVERTED',  # the flight has been diverted to an unplanned airport\n",
                "]\n",
                "\n",
                "columns_to_remove_for_diverted = [\n",
                "    'CANCELLED',  # the flight has been cancelled\n",
                "]\n",
                "\n",
                "names_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "]\n",
                "\n",
                "date_columns_to_convert = [\n",
                "    'FL_DATE'\n",
                "]\n",
                "\n",
                "time_columns_to_convert = [\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME'\n",
                "]\n",
                "\n",
                "numeric_columns_to_convert = [\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "string_columns_to_convert = [\n",
                "    'CANCELLED',\n",
                "    'DIVERTED'\n",
                "]\n",
                "\n",
                "preprocess_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'FL_DATE',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'index'\n",
                "]\n",
                "\n",
                "max_distance = 4970"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_and_save_time(s: str):\n",
                "    time_file.write(s + '\\n')\n",
                "    print(s)\n",
                "\n",
                "\n",
                "def remove_extra_columns(index: str, data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    start_time = tm.time()\n",
                "    oppositeIndex = 'DIVERTED' if index == 'CANCELLED' else 'CANCELLED'\n",
                "    data = data.drop(oppositeIndex)\n",
                "    data = data.drop('index')\n",
                "    data.count()\n",
                "\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Extra column removal concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_strings_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "    udf_string_conversion = udf(lambda x: float(x), DoubleType())\n",
                "    for c in string_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_string_conversion(col(c)))\n",
                "    data.count()\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_names_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    def str_to_float(s: str):\n",
                "        encoding = \"utf-8\"\n",
                "        b = s.encode(encoding)\n",
                "        return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "    udf_names_conversion = udf(lambda x: str_to_float(x), DoubleType())\n",
                "    for c in names_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_names_conversion(col(c)))\n",
                "    data.count()\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_dates_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "    multiplier: float = 1 / 365\n",
                "\n",
                "    def date_to_day_of_year(date_string) -> float:\n",
                "\n",
                "        date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "        day = date.timetuple().tm_yday - 1\n",
                "        return day * multiplier\n",
                "\n",
                "    udf_dates_conversion = udf(\n",
                "        lambda x: date_to_day_of_year(x), DoubleType())\n",
                "    for c in date_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_dates_conversion(col(c)))\n",
                "    data.count()\n",
                "\n",
                "    return data\n",
                "\n",
                "def balance_dataframe(data: ps.DataFrame, index: str, n: int) -> ps.DataFrame:\n",
                "    start_time = tm.time()\n",
                "    positives = data.filter(col(index) == 1)\n",
                "    negatives = data.filter(col(index) == 0)\n",
                "\n",
                "    result = positives.limit(n).\\\n",
                "        union(negatives.limit(n)).\\\n",
                "        orderBy(rand())\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Dataset balancing concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return result\n",
                "\n",
                "def convert_times_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    def time_to_interval(time) -> float:\n",
                "        t = int(float(time))\n",
                "        h = t // 100\n",
                "        m = t % 100\n",
                "        t = h * 60 + m\n",
                "        return float(t / 1140)\n",
                "\n",
                "    udf_time_conversion = udf(lambda x: time_to_interval(x), DoubleType())\n",
                "    for c in time_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_time_conversion(col(c)))\n",
                "    data.count()\n",
                "\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_distance_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    multiplier: float = float(1) / float(max_distance)\n",
                "    udf_numeric_conversion = udf(\n",
                "        lambda x: float(x) * multiplier, DoubleType())\n",
                "\n",
                "    data = data.withColumn(\n",
                "        'DISTANCE', udf_numeric_conversion(col('DISTANCE')))\n",
                "    data.count()\n",
                "    return data\n",
                "\n",
                "\n",
                "def split_data(data: ps.DataFrame, label: str, k: int) -> tuple[ps.DataFrame, ps.DataFrame]:\n",
                "    split_list = []\n",
                "    data = data.dropDuplicates()\n",
                "    print(data.count())\n",
                "\n",
                "    total_positives = data.filter(col(label) == 1).count()\n",
                "    total_negatives = data.filter(col(label) == 0).count()\n",
                "    positives_negatives_ratio = total_positives/total_negatives\n",
                "    k_elements_number = round(data.count() / k)\n",
                "\n",
                "    k_positive_elements = round(\n",
                "        k_elements_number * positives_negatives_ratio)\n",
                "    k_negative_elements = round(\n",
                "        k_elements_number * (1 - positives_negatives_ratio))\n",
                "\n",
                "    i = 0\n",
                "    while i < k:\n",
                "        k_positive_sample = data.where(\n",
                "            col(label) == 1).limit(k_positive_elements)\n",
                "        k_negative_sample = data.where(\n",
                "            col(label) == 0).limit(k_negative_elements)\n",
                "        k_sample = k_positive_sample.union(k_negative_sample)\n",
                "        \n",
                "        print(k_sample.count())\n",
                "        split_list.append(np.array(k_sample.collect()))\n",
                "        data = data.subtract(k_sample)\n",
                "\n",
                "        print(\"Split \" + str(i + 1) + \" of \" + str(k) + \" completed\")\n",
                "        i += 1\n",
                "\n",
                "    return split_list\n",
                "\n",
                "def common_preprocess(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    common_start_time = tm.time()\n",
                "\n",
                "    # Remove rows with Nan key values\n",
                "    data = data.dropna(how='any')\n",
                "\n",
                "    null_removal_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Null values removal concluded: \" +\n",
                "                        str(null_removal_finish_time) + \" seconds\")\n",
                "\n",
                "    data = balance_dataframe(data, problem_to_solve, dataset_limit)\n",
                "\n",
                "    names_start_time = tm.time()\n",
                "    data = convert_names_into_numbers(data)\n",
                "    names_finish_time = tm.time() - names_start_time\n",
                "    print_and_save_time(\"Names conversion concluded: \" +\n",
                "                        str(names_finish_time) + \" seconds\")\n",
                "\n",
                "    dates_start_time = tm.time()\n",
                "    data = convert_dates_into_numbers(data)\n",
                "    dates_finish_time = tm.time() - dates_start_time\n",
                "    print_and_save_time(\"Dates conversion concluded: \" +\n",
                "                        str(dates_finish_time) + \" seconds\")\n",
                "\n",
                "    times_start_time = tm.time()\n",
                "    data = convert_times_into_numbers(data)\n",
                "    times_finish_time = tm.time() - times_start_time\n",
                "    print_and_save_time(\"Times conversion concluded: \" +\n",
                "                        str(times_finish_time) + \" seconds\")\n",
                "\n",
                "    distance_start_time = tm.time()\n",
                "    data = convert_distance_into_numbers(data)\n",
                "    distance_finish_time = tm.time() - distance_start_time\n",
                "    print_and_save_time(\"Distance conversion concluded: \" +\n",
                "                        str(distance_finish_time) + \" seconds\")\n",
                "\n",
                "    strings_start_time = tm.time()\n",
                "    data = convert_strings_into_numbers(data)\n",
                "    strings_finish_time = tm.time() - strings_start_time\n",
                "    print_and_save_time(\"Strings conversion concluded: \" +\n",
                "                        str(strings_finish_time) + \" seconds\")\n",
                "\n",
                "    common_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Common preprocessing concluded: \" +\n",
                "                        str(common_finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "\n",
                "#PREPROCESSING EXECUTION\n",
                "time_file = open(\"./data/times.txt\", \"w\")\n",
                "\n",
                "if not check_preprocessed_data_exists():\n",
                "        download_dataset()\n",
                "\n",
                "        start_time = tm.time()\n",
                "        data = get_dataset(dataset_limit, use_all_dataset_frames)\n",
                "\n",
                "        finish_time = tm.time() - start_time\n",
                "        print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                            str(finish_time) + \" seconds\")\n",
                "\n",
                "        data = common_preprocess(data)\n",
                "        save_dataset(data)\n",
                "else:\n",
                "    data = load_dataset()\n",
                "    udf_string_conversion = udf(lambda x: float(x), DoubleType())\n",
                "    for c in preprocess_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_string_conversion(col(c)))\n",
                "    \n",
                "data = remove_extra_columns(problem_to_solve, data)\n",
                "\n",
                "start_time = tm.time()\n",
                "preprocessing_splits = split_data(data, problem_to_solve, number_of_splits)\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")\n",
                "\n",
                "# Preprocess\n",
                "\n",
                "## Da aggiungere Z score normalization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bonus: Pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pandas_load_dataset():\n",
                "    data = pd.read_csv(filepath_or_buffer=path + '/' + 'preprocessed.csv')\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def pandas_save_dataset(data):\n",
                "    data.to_csv(path_or_buf=path + '/' + 'preprocessed.csv', index=False)\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def pandas_get_dataset(limit: float = -1, allFrames: bool = True):\n",
                "    files = os.listdir(path)\n",
                "    big_frame = pd.DataFrame()\n",
                "\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            frame = pd.read_csv(filepath_or_buffer=path +\n",
                "                                '/' + f, usecols=columns_to_get)\n",
                "            if limit != -1:\n",
                "                frame = frame.sample(n=limit, replace=False)\n",
                "            big_frame = pd.concat([big_frame, frame])\n",
                "    return big_frame\n",
                "\n",
                "\n",
                "def pandas_remove_extra_columns(index: str, data: pd.DataFrame) -> pd.DataFrame:\n",
                "\n",
                "    start_time = tm.time()\n",
                "    oppositeIndex = 'DIVERTED' if index == 'CANCELLED' else 'CANCELLED'\n",
                "    data.drop(oppositeIndex, axis=1, inplace=True)\n",
                "\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Extra column removal concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_balance_dataframe(data: pd.DataFrame, index: str, n: int) -> pd.DataFrame:\n",
                "    start_time = tm.time()\n",
                "    positives = data.query(index + ' == 1')\n",
                "    negatives = data.query(index + ' == 0')\n",
                "    \n",
                "    result = pd.concat([positives.sample(n), negatives.sample(n)]).sample(frac=1)\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Dataset balancing concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return result\n",
                "\n",
                "\n",
                "def pandas_convert_names_into_numbers(data: pd.DataFrame) -> pd.DataFrame:\n",
                "\n",
                "    def str_to_float(s: str):\n",
                "        encoding = \"utf-8\"\n",
                "        b = s.encode(encoding)\n",
                "        return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "    for c in names_columns_to_convert:\n",
                "        data[c] = data[c].apply(str_to_float)\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_convert_dates_into_numbers(data: pd.DataFrame) -> pd.DataFrame:\n",
                "    multiplier: float = 1 / 365\n",
                "\n",
                "    def date_to_day_of_year(date_string) -> float:\n",
                "        date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "        day = date.timetuple().tm_yday - 1\n",
                "        return day * multiplier\n",
                "\n",
                "    for i in date_columns_to_convert:\n",
                "        data[i] = data[i].apply(date_to_day_of_year)\n",
                "\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_convert_times_into_numbers(data: pd.DataFrame) -> pd.DataFrame:\n",
                "\n",
                "    def time_to_interval(time) -> float:\n",
                "        t = int(float(time))\n",
                "        h = t // 100\n",
                "        m = t % 100\n",
                "        t = h * 60 + m\n",
                "        return float(t / 1140)\n",
                "\n",
                "    for c in time_columns_to_convert:\n",
                "        data[c] = data[c].apply(time_to_interval)\n",
                "\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_convert_distance_into_numbers(data: pd.DataFram) -> pd.DataFrame:\n",
                "\n",
                "    multiplier: float = float(1) / float(max_distance)\n",
                "\n",
                "    for c in numeric_columns_to_convert:\n",
                "        data[c] = data[c].apply(lambda x: x * multiplier)\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_split_data(data: pd.DataFrame, label: str, k: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    split_list = []\n",
                "\n",
                "    data.drop_duplicates(inplace=True)\n",
                "\n",
                "    data_positives = data.query(label + ' == 1')\n",
                "    data_negatives = data.query(label + ' == 0')\n",
                "\n",
                "    total_positives = len(data_positives)\n",
                "    total_negatives = len(data_negatives)\n",
                "    positives_negatives_ratio = total_positives/total_negatives\n",
                "\n",
                "    k_elements_number = round(len(data) / k)\n",
                "\n",
                "    k_positive_elements = round(\n",
                "        k_elements_number * positives_negatives_ratio)\n",
                "    k_negative_elements = round(\n",
                "        k_elements_number * (1 - positives_negatives_ratio))\n",
                "\n",
                "    for i in range(1, k + 1):\n",
                "        k_positive_sample = data_positives.head(k_positive_elements)\n",
                "        k_negative_sample = data_negatives.head(k_negative_elements)\n",
                "        k_sample = pd.concat([k_positive_sample, k_negative_sample])\n",
                "\n",
                "        split_list.append(k_sample.to_numpy())\n",
                "        data_positives = data_positives.drop(k_positive_sample.index)\n",
                "        data_negatives = data_negatives.drop(k_negative_sample.index)\n",
                "\n",
                "    return split_list\n",
                "\n",
                "def pandas_common_preprocess(data: ps.DataFrame | pd.DataFrame) -> ps.DataFrame | pd.DataFrame:\n",
                "\n",
                "    common_start_time = tm.time()\n",
                "\n",
                "    data.dropna(how='any', axis='index', inplace=True)\n",
                "\n",
                "    null_removal_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Null values removal concluded: \" +\n",
                "                        str(null_removal_finish_time) + \" seconds\")\n",
                "\n",
                "    pandas_balance_dataframe(data, problem_to_solve, dataset_limit)\n",
                "\n",
                "    names_start_time = tm.time()\n",
                "    data = pandas_convert_names_into_numbers(data)\n",
                "    names_finish_time = tm.time() - names_start_time\n",
                "    print_and_save_time(\"Names conversion concluded: \" +\n",
                "                        str(names_finish_time) + \" seconds\")\n",
                "\n",
                "    dates_start_time = tm.time()\n",
                "    data = pandas_convert_dates_into_numbers(data)\n",
                "    dates_finish_time = tm.time() - dates_start_time\n",
                "    print_and_save_time(\"Dates conversion concluded: \" +\n",
                "                        str(dates_finish_time) + \" seconds\")\n",
                "\n",
                "    times_start_time = tm.time()\n",
                "    data = pandas_convert_times_into_numbers(data)\n",
                "    times_finish_time = tm.time() - times_start_time\n",
                "    print_and_save_time(\"Times conversion concluded: \" +\n",
                "                        str(times_finish_time) + \" seconds\")\n",
                "\n",
                "    distance_start_time = tm.time()\n",
                "    data = pandas_convert_distance_into_numbers(data)\n",
                "    distance_finish_time = tm.time() - distance_start_time\n",
                "    print_and_save_time(\"Distance conversion concluded: \" +\n",
                "                        str(distance_finish_time) + \" seconds\")\n",
                "\n",
                "    common_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Common preprocessing concluded: \" +\n",
                "                        str(common_finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "if not check_preprocessed_data_exists():\n",
                "        download_dataset()\n",
                "\n",
                "        start_time = tm.time()\n",
                "        data = get_dataset(dataset_limit, use_all_dataset_frames)\n",
                "\n",
                "        finish_time = tm.time() - start_time\n",
                "        print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                            str(finish_time) + \" seconds\")\n",
                "\n",
                "        data = pandas_common_preprocess(data)\n",
                "        pandas_save_dataset(data)\n",
                "else:\n",
                "    data = pandas_load_dataset()\n",
                "    \n",
                "data = pandas_remove_extra_columns(problem_to_solve, data)\n",
                "\n",
                "start_time = tm.time()\n",
                "preprocessing_splits = pandas_split_data(data, problem_to_solve, number_of_splits)\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generic Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    '''\n",
                "    Calculates the sigmoid of the given data\n",
                "    '''\n",
                "    g = 1.0 / (1.0 + np.exp(-x))\n",
                "    return g\n",
                "\n",
                "def binary_cross_entropy(y, y_label, w, l2):\n",
                "    '''\n",
                "    Calculates the binary cross entropy loss of the calculated y and the given y_label\n",
                "    '''\n",
                "    loss = -np.mean(y_label*(np.log(y)) + (1-y_label)\n",
                "                    * np.log(1-y)) + regularize(w, l2)\n",
                "    return loss\n",
                "\n",
                "def regularize(W, l2):\n",
                "    '''\n",
                "    Calculates the regularization term for the loss\n",
                "    '''\n",
                "    return (l2 / 2) * np.sum(np.square(W))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parallel Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ParallelLogisticRegression:\n",
                "    iterations: int\n",
                "    learning_rate: float\n",
                "    batch_size: int\n",
                "    l2: float\n",
                "    W: Broadcast\n",
                "    b: float\n",
                "\n",
                "def initialize(self: ParallelLogisticRegression, size):\n",
                "    self.W = context.broadcast(np.random.rand(size))\n",
                "    self.b = np.random.rand()\n",
                "\n",
                "\n",
                "def train(self: ParallelLogisticRegression, data: ps.DataFrame):\n",
                "    initialize(self, len(data.columns) - 1)\n",
                "\n",
                "    num_chunks = X.count() // self.batch_size\n",
                "    chunk_percent = 1/num_chunks\n",
                "\n",
                "    batches = data.randomSplit([chunk_percent] * num_chunks)\n",
                "\n",
                "    Y_labels = [b.select(problem_to_solve).rdd for b in batches]\n",
                "    X = [b.drop(problem_to_solve).rdd for b in batches]\n",
                "\n",
                "    losses = []\n",
                "    gradients = []\n",
                "\n",
                "    for _ in range(self.iterations):\n",
                "        _losses = []\n",
                "        _gradients = [] \n",
                "\n",
                "        for b_X, b_Y_labels in zip(X, Y_labels):\n",
                "            Y = self.evaluate(b_X)\n",
                "            _losses.append(binary_cross_entropy(\n",
                "                Y, b_Y_labels, self.W, self.l2))\n",
                "            (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "            _gradients.append(dW)\n",
                "            self.update(dW, db)\n",
                "        losses.append(np.mean(_losses))\n",
                "        gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)\n",
                "\n",
                "def evaluate(self: ParallelLogisticRegression, X: RDD):\n",
                "    Z = X.map(lambda x: np.dot(x, self.W.value)).reduce(lambda a, b: a+b + self.b)\n",
                "    Z = Z.map(lambda x: sigmoid(x))\n",
                "    return Z\n",
                "        \n",
                "def update(self, dW: list[float], db: float):\n",
                "        self.W = context.broadcast(self.W.value - self.learning_rate * dW)\n",
                "        self.b = self.b - self.learning_rate * db"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Serial Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SerialLogisticRegression():\n",
                "    def __init__(self, iterations: int, learning_rate: float, batch_size: int, l2: float):\n",
                "        self.iterations = iterations\n",
                "        self.learning_rate = learning_rate\n",
                "        self.batch_size = batch_size\n",
                "        self.l2 = l2\n",
                "\n",
                "    def initialize(self, columns_number):\n",
                "        self.W = np.random.rand(columns_number)\n",
                "        self.b = np.random.rand()\n",
                "\n",
                "    def evaluate(self, X):\n",
                "        Z = np.dot(X, self.W) + self.b\n",
                "        Z = sigmoid(Z)\n",
                "        return Z\n",
                "\n",
                "    def gradient(self, X, Y, Y_label):\n",
                "        '''\n",
                "        Calculates the gradient w.r.t weights and bias\n",
                "        '''\n",
                "\n",
                "        # Number of training examples.\n",
                "        m = X.shape[0]\n",
                "\n",
                "        # Gradient of loss w.r.t weights with regularization\n",
                "        dw = (1/m)*np.dot(X.T, (Y - Y_label)) + self.l2 * self.W\n",
                "\n",
                "        # Gradient of loss w.r.t bias with regularization\n",
                "        db = (1/m)*np.sum((Y - Y_label))\n",
                "\n",
                "        return dw, db\n",
                "\n",
                "    def update(self, dW, db):\n",
                "        self.W = self.W - self.learning_rate * dW\n",
                "        self.b = self.b - self.learning_rate * db\n",
                "\n",
                "    def train(self, X, Y_labels):\n",
                "        self.initialize(X.shape[1])\n",
                "        losses = []\n",
                "        gradients = []\n",
                "\n",
                "        for _ in range(self.iterations):\n",
                "            _losses = []\n",
                "            _gradients = []\n",
                "            for b in range(X.shape[0]//self.batch_size):\n",
                "                b_X = X[b*self.batch_size:b*self.batch_size+self.batch_size, :]\n",
                "                b_Y_labels = Y_labels[b*self.batch_size:b *\n",
                "                                      self.batch_size+self.batch_size]\n",
                "                Y = self.evaluate(b_X)\n",
                "                _losses.append(binary_cross_entropy(\n",
                "                    Y, b_Y_labels, self.W, self.l2))\n",
                "                (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "                _gradients.append(dW)\n",
                "                self.update(dW, db)\n",
                "            losses.append(np.mean(_losses))\n",
                "            gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiments\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_roc(labels, results, name):\n",
                "    labels_and_results = sorted(\n",
                "        list(zip(labels, map(lambda x: x, results))), key=lambda x: x[1])\n",
                "\n",
                "    labels_by_weights = np.array([k for (k, _) in labels_and_results])\n",
                "\n",
                "    length = labels_by_weights.size\n",
                "\n",
                "    true_positives = labels_by_weights.cumsum()\n",
                "\n",
                "    num_positive = true_positives[-1]\n",
                "\n",
                "    false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
                "\n",
                "    true_positives_rate = true_positives / num_positive\n",
                "    false_positives_rate = false_positives / (length - num_positive)\n",
                "\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
                "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
                "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
                "    plt.plot(false_positives_rate, true_positives_rate,\n",
                "             color='#8cbfd0', linestyle='-', linewidth=3.)\n",
                "    plt.plot((0., 1.), (0., 1.), linestyle='--',\n",
                "             color='#d6ebf2', linewidth=2.)\n",
                "\n",
                "    plt.savefig('./data/{}_roc.png'.format(name))\n",
                "    fig.clear()\n",
                "    plt.close()\n",
                "\n",
                "def plot_loss_gradient(iterations, train_losses, gradients, name):\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlabel('Iterations')\n",
                "    ax.set_ylabel('Loss/Gradient')\n",
                "    ax.set_title(name)\n",
                "    ax.plot(range(iterations), train_losses, label='Loss')\n",
                "    ax.plot(range(iterations), gradients, label='Gradient')\n",
                "    ax.grid()\n",
                "    ax.legend()\n",
                "\n",
                "    fig.savefig(\"./data/{}.png\".format(name))\n",
                "    fig.clear()\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparamters Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(100, 0.001, 0),\n",
                            " (100, 0.001, 0.1),\n",
                            " (100, 0.001, 0.001),\n",
                            " (100, 0.01, 0),\n",
                            " (100, 0.01, 0.1),\n",
                            " (100, 0.01, 0.001),\n",
                            " (100, 0.1, 0),\n",
                            " (100, 0.1, 0.1),\n",
                            " (100, 0.1, 0.001),\n",
                            " (200, 0.001, 0),\n",
                            " (200, 0.001, 0.1),\n",
                            " (200, 0.001, 0.001),\n",
                            " (200, 0.01, 0),\n",
                            " (200, 0.01, 0.1),\n",
                            " (200, 0.01, 0.001),\n",
                            " (200, 0.1, 0),\n",
                            " (200, 0.1, 0.1),\n",
                            " (200, 0.1, 0.001),\n",
                            " (500, 0.001, 0),\n",
                            " (500, 0.001, 0.1),\n",
                            " (500, 0.001, 0.001),\n",
                            " (500, 0.01, 0),\n",
                            " (500, 0.01, 0.1),\n",
                            " (500, 0.01, 0.001),\n",
                            " (500, 0.1, 0),\n",
                            " (500, 0.1, 0.1),\n",
                            " (500, 0.1, 0.001)]"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grid = { 'iter': [100, 200, 500], 'lr': [0.001, 0.01, 0.1], 'l2': [0, 0.1, 0.001], 'batch_size': [0, 20]}\n",
                "\n",
                "params = list(itertools.product(*grid.values()))\n",
                "\n",
                "params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "class Model:\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        self.iterations = iterations\n",
                "        self.lr = lr\n",
                "        self.l2 = l2\n",
                "        self.batch_size = batch_size\n",
                "\n",
                "    def train(self, data):\n",
                "        total_train_losses = []\n",
                "        total_test_losses = []\n",
                "        for i, fold in enumerate(data):\n",
                "            train_data = data[:i] + data[i + 1:]\n",
                "            test_data = fold\n",
                "            losses,  = self.train_impl(train_data)\n",
                "            total_train_losses.append(losses) \n",
                "            loss = self.test_impl(test_data)\n",
                "            total_test_losses.append(loss)\n",
                "        return total_train_losses, total_test_losses\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.5 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.5"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e51fdbc37c5adaa12a9ed97b50fdaf0ff46be7ee1a114cc405b6eaf0c36d1bf8"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
