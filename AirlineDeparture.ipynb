{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!sudo apt update\n",
                "#!apt-get install openjdk-8-jdk-headless - qq > /dev/null\n",
                "#!wget - q https: // dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop2.tgz\n",
                "#!tar xf spark-3.3.0-bin-hadoop2.tgz\n",
                "#!pip install - r requirements.txt\n",
                "\n",
                "import os\n",
                "from kaggle.api.kaggle_api_extended import KaggleApi\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import monotonically_increasing_id, col, udf, rand\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "import pyspark.sql as ps\n",
                "from zlib import crc32\n",
                "import time as tm\n",
                "from datetime import datetime as dt\n",
                "import itertools\n",
                "from dataclasses import dataclass\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.rdd import RDD\n",
                "from pyspark.broadcast import Broadcast\n",
                "import findspark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = './data'\n",
                "worker_nodes = \"*\"\n",
                "problem_to_solve = 'CANCELLED'\n",
                "\n",
                "dataset_limit = 100000\n",
                "use_all_dataset_frames = True\n",
                "fold_number = 10\n",
                "load_cached = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DA SCRIVERE\n",
                "- perche' usiamo i dataframe invece degli rdd nella prima parte\n",
                "- aggiungere k fold cross validation\n",
                "- aggiungere griglia parametri\n",
                "- aggiungere label stratification\n",
                "- aggiungere performance modello pyspark\n",
                "- aggiungere check e info extra su dataset di base (es sbilanciamento)\n",
                "- auroc, auprc, f1, \n",
                "- confronto con tree classifier\n",
                "- confrontare ogni pezzo con MLLib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['KAGGLE_USERNAME'] = \"davidetricella\"\n",
                "os.environ['KAGGLE_KEY'] = \"e1ab3aae4a07f36b37a3a8bace74d9df\"\n",
                "\n",
                "\n",
                "dataset = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
                "path = './data'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_dataset():\n",
                "    if not os.path.isdir(path):\n",
                "        os.mkdir(path)\n",
                "    if not os.listdir(path):\n",
                "        try:\n",
                "            api = KaggleApi()\n",
                "            api.authenticate()\n",
                "            api.dataset_download_files(dataset, path, unzip=True, quiet=False)\n",
                "        except:\n",
                "            print(\"Error downloading the dataset\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataframe_schema = StructType([\n",
                "    StructField('FL_DATE', StringType(), True),\n",
                "    StructField('OP_CARRIER', StringType(), True),\n",
                "    StructField('ORIGIN', StringType(), True),\n",
                "    StructField('DEST', StringType(), True),\n",
                "    StructField('CRS_DEP_TIME', StringType(), True),\n",
                "    StructField('CRS_ARR_TIME', StringType(), True),\n",
                "    StructField('CANCELLED', StringType(), True),\n",
                "    StructField('DIVERTED', StringType(), True),\n",
                "    StructField('CRS_ELAPSED_TIME', StringType(), True),\n",
                "    StructField('DISTANCE', StringType(), True)\n",
                "])\n",
                "\n",
                "columns_to_get = [\n",
                "    'FL_DATE',\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "findspark.init()\n",
                "findspark.find()\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                ".appName(\"Airline Departure\") \\\n",
                ".master('local[' + worker_nodes + ']') \\\n",
                ".getOrCreate()\n",
                "\n",
                "context = spark.sparkContext"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset():\n",
                "    data = spark.read.format(\"csv\") \\\n",
                "        .option(\"header\", True) \\\n",
                "        .load('./preprocessed/' + problem_to_solve)\n",
                "\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def save_dataset(data):\n",
                "    data.write.format('csv').option('header', True).mode('overwrite').option(\n",
                "        'sep', ',').save('./preprocessed/' + problem_to_solve)\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def check_preprocessed_data_exists() -> bool:\n",
                "    files = os.listdir('./data')\n",
                "    for f in files:\n",
                "        if f.startswith('preprocessed'):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def get_dataset(limit: float = -1, allFrames: bool = True):\n",
                "    files = os.listdir(path)\n",
                "    big_frame = spark.createDataFrame(\n",
                "        spark.sparkContext.emptyRDD(), schema=dataframe_schema)\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            frame = spark.read.option(\"header\", True).csv(path + '/' + f)\n",
                "            frame = frame.select(columns_to_get)\n",
                "            frame = frame.orderBy(rand())\n",
                "\n",
                "            if limit != -1:\n",
                "                frame = frame.limit(limit)\n",
                "\n",
                "            big_frame = frame.union(big_frame)\n",
                "\n",
                "    big_frame = big_frame.withColumn(\n",
                "        \"id\", monotonically_increasing_id()).orderBy(rand())\n",
                "    big_frame.count()\n",
                "\n",
                "    return big_frame\n",
                "\n",
                "def print_and_save_time(s: str):\n",
                "  #time_file.write(s + '\\n')\n",
                "  print(s)\n",
                "#time_file = open(\"./output/times.txt\", mode= \"w\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "columns_to_remove_for_canceled = [\n",
                "    'DIVERTED',  # the flight has been diverted to an unplanned airport\n",
                "]\n",
                "\n",
                "columns_to_remove_for_diverted = [\n",
                "    'CANCELLED',  # the flight has been cancelled\n",
                "]\n",
                "\n",
                "preprocess_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'FL_DATE',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'index'\n",
                "]\n",
                "\n",
                "max_distance = 4970"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Charts Plotting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_balancing_chart(data: ps.DataFrame, label: str):\n",
                "  total_positives = data.filter(col(label) == 1).count()\n",
                "  total_negatives = data.filter(col(label) == 0).count()\n",
                "  fig, ax = plt.subplots()\n",
                "\n",
                "  labels = ['REGULAR', label]\n",
                "  counts = [total_negatives, total_positives]\n",
                "  bar_colors = ['tab:blue', 'tab:red']\n",
                "\n",
                "  ax.bar(labels, counts, color=bar_colors)\n",
                "\n",
                "  ax.set_ylabel('Counts')\n",
                "  ax.set_title('Regular flights and problematic flights counts')\n",
                "\n",
                "  plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Dataset Reading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "download_dataset()\n",
                "start_time = tm.time()\n",
                "data = get_dataset(dataset_limit, use_all_dataset_frames).cache()\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Null Rows Dropping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "common_start_time = tm.time()\n",
                "\n",
                "data = data.dropna(how='any')\n",
                "print(\"Dataframe rows after NaN dropping: \" + str(data.count()))\n",
                "\n",
                "null_removal_finish_time = tm.time() - common_start_time\n",
                "print_and_save_time(\"Null values removal concluded: \" +\n",
                "                    str(null_removal_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Dataframe Balancing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_time = tm.time()\n",
                "irregular_flights = data.filter(col(problem_to_solve) == 1)\n",
                "\n",
                "regular_flights = data.filter(col(problem_to_solve) == 0).limit(irregular_flights.count())\n",
                "\n",
                "flight_ids = irregular_flights.rdd.map(lambda x: x.id).collect() + \\\n",
                "    regular_flights.rdd.map(lambda x: x.id).collect()\n",
                "\n",
                "data = data.filter(data.id.isin(flight_ids)).orderBy(rand())\n",
                "print(\"Balanced dataframe rows: \" + str(data.count()))\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset balancing concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")\n",
                "\n",
                "plot_balancing_chart(data, problem_to_solve)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Column Conversions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "columns_start_time = tm.time()\n",
                "\n",
                "@udf(returnType=DoubleType())\n",
                "def str_to_float(s: str):\n",
                "  encoding = \"utf-8\"\n",
                "  b = s.encode(encoding)\n",
                "  return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "date_multiplier: float = 1 / 365\n",
                "@udf(returnType=DoubleType())\n",
                "def date_to_day_of_year(date_string) -> float:\n",
                "  date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "  day = date.timetuple().tm_yday - 1\n",
                "  return day * date_multiplier\n",
                "\n",
                "@udf(returnType=DoubleType())\n",
                "def time_to_interval(time) -> float:\n",
                "  t = int(float(time))\n",
                "  h = t // 100\n",
                "  m = t % 100\n",
                "  t = h * 60 + m\n",
                "  return float(t / 1140)\n",
                "\n",
                "distance_multiplier = float(1) / float(max_distance)\n",
                "\n",
                "data = data.select(\n",
                "  (data.CANCELLED.cast('double')).alias(\"CANCELLED\"),\n",
                "  (data.DIVERTED.cast('double')).alias(\"DIVERTED\"),\n",
                "  str_to_float(data.OP_CARRIER).alias(\"OP_CARRIER\"),\n",
                "  str_to_float(data.ORIGIN).alias(\"ORIGIN\"),\n",
                "  str_to_float(data.DEST).alias(\"DEST\"),\n",
                "  date_to_day_of_year(data.FL_DATE).alias(\"FL_DATE\"),\n",
                "  time_to_interval(data.CRS_DEP_TIME).alias(\"CRS_DEP_TIME\"),\n",
                "  time_to_interval(data.CRS_ARR_TIME).alias(\"CRS_ARR_TIME\"),\n",
                "  time_to_interval(data.CRS_ELAPSED_TIME).alias(\"CRS_ELAPSED_TIME\"),\n",
                "  (data.DISTANCE.cast('double') * distance_multiplier).alias(\"DISTANCE\"),\n",
                "  data.id\n",
                ")\n",
                "\n",
                "data.count()\n",
                "\n",
                "columns_finish_time = tm.time() - columns_start_time\n",
                "print_and_save_time(\"Columns conversion concluded: \" +\n",
                "                    str(columns_finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Z Score Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "z_start_time = tm.time()\n",
                "column_list = data.columns\n",
                "column_mean_dict = dict()\n",
                "column_stddv_dict = dict()\n",
                "\n",
                "for c in column_list:\n",
                "    column_mean_dict[c] = data.agg({c: 'mean'}).head()[0]\n",
                "    column_stddv_dict[c] = data.agg({c: 'stddev'}).head()[0]\n",
                "\n",
                "data = data.select(\n",
                "  problem_to_solve,\n",
                "\n",
                "  ((data.OP_CARRIER - column_mean_dict[\"OP_CARRIER\"]) / column_stddv_dict[\"OP_CARRIER\"]).alias('OP_CARRIER'),\n",
                "\n",
                "  ((data.ORIGIN - column_mean_dict[\"ORIGIN\"]) / column_stddv_dict[\"ORIGIN\"]).alias('ORIGIN'),\n",
                "\n",
                "  ((data.DEST - column_mean_dict[\"DEST\"]) / column_stddv_dict[\"DEST\"]).alias('DEST'),\n",
                "\n",
                "  ((data.FL_DATE - column_mean_dict[\"FL_DATE\"]) / column_stddv_dict[\"FL_DATE\"]).alias('FL_DATE'),\n",
                "\n",
                "  ((data.CRS_DEP_TIME - column_mean_dict[\"CRS_DEP_TIME\"]) / column_stddv_dict[\"CRS_DEP_TIME\"]).alias('CRS_DEP_TIME'),\n",
                "\n",
                "  ((data.CRS_ARR_TIME - column_mean_dict[\"CRS_ARR_TIME\"]) /  column_stddv_dict[\"CRS_ARR_TIME\"]).alias('CRS_ARR_TIME'),\n",
                "\n",
                "  ((data.CRS_ELAPSED_TIME - column_mean_dict[\"CRS_ELAPSED_TIME\"]) / column_stddv_dict[\"CRS_ELAPSED_TIME\"]).alias('CRS_ELAPSED_TIME'),\n",
                "\n",
                "  ((data.DISTANCE - column_mean_dict[\"DISTANCE\"]) / column_stddv_dict[\"DISTANCE\"]).alias('DISTANCE'),\n",
                "\n",
                "  data.id\n",
                ")\n",
                "\n",
                "data.count()\n",
                "\n",
                "z_finish_time = tm.time() - z_start_time\n",
                "print_and_save_time(\"Z score normalization concluded: \" +\n",
                "                    str(z_finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Preprocessed dataset Saving/Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_dataset(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if load_cached:\n",
                "    data = load_dataset().cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Data Splitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_time = tm.time()\n",
                "folds = []\n",
                "\n",
                "k_elements_half_number = math.floor((data.count() / fold_number) / 2)\n",
                "\n",
                "i = 0\n",
                "while i < fold_number:\n",
                "    k_positives = data.where(\n",
                "        col(problem_to_solve) == 1).limit(k_elements_half_number)\n",
                "\n",
                "    k_negatives = data.where(\n",
                "        col(problem_to_solve) == 0).limit(k_elements_half_number)\n",
                "\n",
                "    k_ids = k_positives.rdd.map(lambda x: x.id).collect() + \\\n",
                "        k_negatives.rdd.map(lambda x: x.id).collect()\n",
                "\n",
                "    k_sample = data.filter(data.id.isin(k_ids))\n",
                "    k_sample = k_sample.drop(k_sample.id)\n",
                "\n",
                "    folds.append(k_sample)\n",
                "    data = data.filter(~data.id.isin(k_ids))\n",
                "\n",
                "    print(\"Split \" + str(i + 1) + \" of \" + str(fold_number) + \" completed\")\n",
                "    print(\"Dataframe rows: \" + str(data.count()))\n",
                "    i += 1\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Bonus: Pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pandas_save_dataset(data):\n",
                "    data.to_csv(path_or_buf=path + '/' + 'preprocessed.csv', index=False)\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "# Data Load\n",
                "\n",
                "files = os.listdir(path)\n",
                "data = pd.DataFrame()\n",
                "\n",
                "for f in files:\n",
                "    if f.endswith('.csv'):\n",
                "        frame = pd.read_csv(filepath_or_buffer=path +\n",
                "                            '/' + f, usecols=columns_to_get)\n",
                "        frame.sample(frac=1)\n",
                "        frame = frame.head(dataset_limit)\n",
                "        data = pd.concat([data, frame])\n",
                "\n",
                "data = data.dropna(how='any', axis='index')\n",
                "print(\"Dataset acquisition completed\")\n",
                "\n",
                "# Problem Selection\n",
                "\n",
                "irregulars = data.loc[data[problem_to_solve] == 1]\n",
                "regulars = data.loc[data[problem_to_solve] == 0]\n",
                "\n",
                "data = pd.concat([regulars.sample(len(irregulars)), irregulars]).sample(frac=1)\n",
                "\n",
                "oppositeIndex = 'DIVERTED' if problem_to_solve == 'CANCELLED' else 'CANCELLED'\n",
                "data = data.drop(oppositeIndex, axis=1)\n",
                "print(\"Dataset balancing completed\")\n",
                "\n",
                "# Names Conversion\n",
                "\n",
                "def str_to_float(s: str):\n",
                "    encoding = \"utf-8\"\n",
                "    b = s.encode(encoding)\n",
                "    return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "for c in ['OP_CARRIER', 'ORIGIN', 'DEST']:\n",
                "    data[c] = data[c].apply(str_to_float)\n",
                "\n",
                "# Dates Conversion\n",
                "\n",
                "multiplier: float = 1 / 365\n",
                "\n",
                "def date_to_day_of_year(date_string) -> float:\n",
                "    date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "    day = date.timetuple().tm_yday - 1\n",
                "    return day * multiplier\n",
                "\n",
                "data[\"FL_DATE\"] = data[\"FL_DATE\"].apply(date_to_day_of_year)\n",
                "\n",
                "# Time Conversion\n",
                "    \n",
                "def time_to_interval(time) -> float:\n",
                "    t = int(float(time))\n",
                "    h = t // 100\n",
                "    m = t % 100\n",
                "    t = h * 60 + m\n",
                "    return float(t / 1140)\n",
                "\n",
                "for c in [\"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\"]:\n",
                "    data[c] = data[c].apply(time_to_interval)\n",
                "\n",
                "# Distance Conversion\n",
                "    \n",
                "multiplier: float = float(1) / float(max_distance)\n",
                "\n",
                "data[\"DISTANCE\"] = data[\"DISTANCE\"].apply(lambda x: x * multiplier)\n",
                "\n",
                "print(\"Dataset conversions completed\")\n",
                "\n",
                "#Z-score normalization\n",
                "\n",
                "def z_score_normalize(x, m, s) -> float:\n",
                "    return (x - m) / s\n",
                "\n",
                "column_list = list(data)\n",
                "column_list.remove(problem_to_solve)\n",
                "\n",
                "for c in column_list:\n",
                "    column_mean = data[c].mean()\n",
                "    column_stddv = data[c].std()\n",
                "    data[c] = data[c].apply(z_score_normalize, args=(column_mean, column_stddv))\n",
                "\n",
                "print(\"Dataset normalization completed\")\n",
                "# Create Folds\n",
                "\n",
                "folds = []\n",
                "\n",
                "data.drop_duplicates(inplace=True)\n",
                "\n",
                "irregulars = data.loc[data[problem_to_solve] == 1]\n",
                "regulars = data.loc[data[problem_to_solve] == 0]\n",
                "\n",
                "k_elements_half_number = round((len(data) / fold_number) / 2)\n",
                "\n",
                "for i in range(1, fold_number + 1):\n",
                "    k_irregulars_sample = irregulars.head(k_elements_half_number)\n",
                "    k_regulars_sample = regulars.head(k_elements_half_number)\n",
                "    k_sample = pd.concat([k_irregulars_sample, k_regulars_sample])\n",
                "\n",
                "    folds.append(k_sample)\n",
                "    irregulars = irregulars.drop(k_irregulars_sample.index)\n",
                "    regulars = regulars.drop(k_regulars_sample.index)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generic Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    '''\n",
                "    Calculates the sigmoid of the given data\n",
                "    '''\n",
                "    g = 1.0 / (1.0 + np.exp(-x))\n",
                "    return g\n",
                "\n",
                "def binary_cross_entropy(y, y_label, w, l2):\n",
                "    '''\n",
                "    Calculates the binary cross entropy loss of the calculated y and the given y_label\n",
                "    '''\n",
                "    loss = -np.mean(y_label*(np.log(y)) + (1-y_label)\n",
                "                    * np.log(1-y)) + regularize(w, l2)\n",
                "    return loss\n",
                "\n",
                "def regularize(W, l2):\n",
                "    '''\n",
                "    Calculates the regularization term for the loss\n",
                "    '''\n",
                "    return (l2 / 2) * np.sum(np.square(W))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parallel Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ParallelLogisticRegression:\n",
                "    iterations: int\n",
                "    learning_rate: float\n",
                "    batch_size: int\n",
                "    l2: float\n",
                "    W: Broadcast\n",
                "    b: float\n",
                "\n",
                "def parallel_initialize(self: ParallelLogisticRegression, feature_number: int):\n",
                "    self.W = context.broadcast(np.random.rand(feature_number))\n",
                "    self.b = np.random.rand()\n",
                "\n",
                "def parallel_train(self: ParallelLogisticRegression, data: ps.DataFrame):\n",
                "\n",
                "    if self.batch_size != 0:\n",
                "        num_chunks = data.count() // self.batch_size\n",
                "        chunk_percent = 1/num_chunks\n",
                "        batches = data.randomSplit([chunk_percent] * num_chunks)\n",
                "    else:\n",
                "        batches = [data]\n",
                "\n",
                "    batches_rdd = [b.rdd for b in batches]\n",
                "\n",
                "    losses = []\n",
                "    gradients = []\n",
                "\n",
                "    for _ in range(self.iterations):\n",
                "        _losses = []\n",
                "        _gradients = []\n",
                "\n",
                "        for batch in batches_rdd:\n",
                "            batch = format_rdd(batch).cache()\n",
                "            Y = parallel_evaluate(self, batch.map(lambda x: x[1]))\n",
                "            _losses.append(parallel_binary_cross_entropy(self, batch.map(lambda x: x[0]).zip(Y)))\n",
                "            (dW, db) = parallel_gradient(self, batch, Y)\n",
                "            _gradients.append(dW)\n",
                "            parallel_update(self, dW, db)\n",
                "        losses.append(np.mean(_losses))\n",
                "        gradients.append(np.mean(_gradients))\n",
                "\n",
                "    return (losses, gradients)\n",
                "\n",
                "\n",
                "def parallel_evaluate(self: ParallelLogisticRegression, X: RDD) -> RDD:\n",
                "    Z: RDD = X.map(lambda x: np.dot(x, self.W.value))#.reduce(lambda a, b: a+b + self.b)\n",
                "    Z = Z.map(lambda x: sigmoid(x))\n",
                "    return Z\n",
                "\n",
                "def parallel_binary_cross_entropy(self: ParallelLogisticRegression, X_Y: RDD)-> float:\n",
                "    L: RDD = X_Y.map(lambda y: y[0] * np.log(y[1]) + (1 - y[0]) * np.log(1 -  y[1]))\n",
                "    return -L.reduce(lambda a, b: a + b)/L.count() + regularize(self.W.value, self.l2)\n",
                "\n",
                "def parallel_gradient(self: ParallelLogisticRegression, X: RDD, Y: RDD)-> tuple[np.ndarray, np.ndarray]:\n",
                "    m = X.count()\n",
                "    dw = X.zip(Y).map(lambda x: np.dot((x[1] - x[0][0]), x[0][1])).reduce(lambda a, b: (a + b) * 1/m + self.W.value * self.l2)\n",
                "    db = X.zip(Y).map(lambda x: x[1] - x[0][0]).reduce(lambda a, b: a + b) * 1/m\n",
                "    return dw, db\n",
                "\n",
                "def parallel_update(self: ParallelLogisticRegression, dW: list[float], db: float):\n",
                "        self.W = context.broadcast(self.W.value - self.learning_rate * dW)\n",
                "        self.b = self.b - self.learning_rate * db\n",
                "\n",
                "def format_rdd(rdd: RDD) -> RDD:\n",
                "    return rdd.map(lambda x: (float(x[0]), [float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5]), float(x[6]), float(x[7]), float(x[8])]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "m = ParallelLogisticRegression(100, 0.01, 0, 0.1, None, None)\n",
                "f: pd.DataFrame = folds[0]\n",
                "\n",
                "parallel_initialize(m, len(f.columns) - 1)\n",
                "parallel_train(m, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Serial Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SerialLogisticRegression():\n",
                "    def __init__(self, iterations: int, learning_rate: float, batch_size: int, l2: float):\n",
                "        self.iterations = iterations\n",
                "        self.learning_rate = learning_rate\n",
                "        self.batch_size = batch_size\n",
                "        self.l2 = l2\n",
                "\n",
                "    def initialize(self, columns_number):\n",
                "        self.W = np.random.rand(columns_number)\n",
                "        self.b = np.random.rand()\n",
                "\n",
                "    def evaluate(self, X):\n",
                "        Z = np.dot(X, self.W) + self.b\n",
                "        Z = sigmoid(Z)\n",
                "        return Z\n",
                "\n",
                "    def gradient(self, X, Y, Y_label):\n",
                "        '''\n",
                "        Calculates the gradient w.r.t weights and bias\n",
                "        '''\n",
                "\n",
                "        # Number of training examples.\n",
                "        m = X.shape[0]\n",
                "\n",
                "        # Gradient of loss w.r.t weights with regularization\n",
                "        dw = (1/m)*np.dot(X.T, (Y - Y_label)) + self.l2 * self.W\n",
                "\n",
                "        # Gradient of loss w.r.t bias with regularization\n",
                "        db = (1/m)*np.sum((Y - Y_label))\n",
                "\n",
                "        return dw, db\n",
                "\n",
                "    def update(self, dW, db):\n",
                "        self.W = self.W - self.learning_rate * dW\n",
                "        self.b = self.b - self.learning_rate * db\n",
                "\n",
                "    def train(self, X, Y_labels, iterations = 10):\n",
                "        self.initialize(X.shape[1])\n",
                "        losses = []\n",
                "        gradients = []\n",
                "\n",
                "        for _ in range(iterations):\n",
                "            _losses = []\n",
                "            _gradients = []\n",
                "            for b in range(X.shape[0]//self.batch_size):\n",
                "                b_X = X[b*self.batch_size:b*self.batch_size+self.batch_size, :]\n",
                "                b_Y_labels = Y_labels[b*self.batch_size:b *\n",
                "                                      self.batch_size+self.batch_size]\n",
                "                Y = self.evaluate(b_X)\n",
                "                _losses.append(binary_cross_entropy(\n",
                "                    Y, b_Y_labels, self.W, self.l2))\n",
                "                (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "                _gradients.append(dW)\n",
                "                self.update(dW, db)\n",
                "            losses.append(np.mean(_losses))\n",
                "            gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiments\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_roc(labels, results, name):\n",
                "    labels_and_results = sorted(\n",
                "        list(zip(labels, map(lambda x: x, results))), key=lambda x: x[1])\n",
                "\n",
                "    labels_by_weights = np.array([k for (k, _) in labels_and_results])\n",
                "\n",
                "    length = labels_by_weights.size\n",
                "\n",
                "    true_positives = labels_by_weights.cumsum()\n",
                "\n",
                "    num_positive = true_positives[-1]\n",
                "\n",
                "    false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
                "\n",
                "    true_positives_rate = true_positives / num_positive\n",
                "    false_positives_rate = false_positives / (length - num_positive)\n",
                "\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
                "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
                "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
                "    plt.plot(false_positives_rate, true_positives_rate,\n",
                "             color='#8cbfd0', linestyle='-', linewidth=3.)\n",
                "    plt.plot((0., 1.), (0., 1.), linestyle='--',\n",
                "             color='#d6ebf2', linewidth=2.)\n",
                "\n",
                "    plt.savefig('./data/{}_roc.png'.format(name))\n",
                "    fig.clear()\n",
                "    plt.close()\n",
                "\n",
                "def plot_loss_gradient(iterations, train_losses, gradients, name):\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlabel('Iterations')\n",
                "    ax.set_ylabel('Loss/Gradient')\n",
                "    ax.set_title(name)\n",
                "    ax.plot(range(iterations), train_losses, label='Loss')\n",
                "    ax.plot(range(iterations), gradients, label='Gradient')\n",
                "    ax.grid()\n",
                "    ax.legend()\n",
                "\n",
                "    fig.savefig(\"./data/{}.png\".format(name))\n",
                "    fig.clear()\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparamters Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid = { 'iter': [100, 200, 500], 'lr': [0.001, 0.01, 0.1], 'l2': [0, 0.1, 0.001], 'batch_size': [0, 20]}\n",
                "\n",
                "params = list(itertools.product(*grid.values()))\n",
                "\n",
                "params"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### K-Fold Cross Validation\n",
                "The following code defines a base class with train and evaluation methods to apply the K-Fold Cross Validation to each model\n",
                "\n",
                "DIVIDIAMOLA IN 10, 8 + 1 NEL TRAINING E 1 PER EVALUATION FINALE?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "class Evaluator:\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        self.iterations = iterations\n",
                "        self.lr = lr\n",
                "        self.l2 = l2\n",
                "        self.batch_size = batch_size\n",
                "\n",
                "    def train(self, data: list[ps.DataFrame]):\n",
                "        total_train_losses = []\n",
                "        total_test_losses = []\n",
                "        for i, fold in enumerate(data):\n",
                "            test_data = fold\n",
                "            remaining_folds = data[:i] + data[i + 1:]\n",
                "            train_data: ps.DataFrame = remaining_folds[0]\n",
                "            for train_fold in remaining_folds[1:]:\n",
                "                train_data = train_data.union(train_fold)\n",
                "            losses, gradient = self.train_impl(train_fold)\n",
                "            total_train_losses.append(losses) \n",
                "            loss = self.test_impl(test_data)\n",
                "            total_test_losses.append(loss)\n",
                "            print(\"Test Loss for fold \" + i + \": \" + loss)\n",
                "        return total_train_losses, total_test_losses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ParallelModelEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = ParallelLogisticRegression(self.iterations, self.lr, self.batch_size, self.l2, None, None)\n",
                "        parallel_initialize(self.model, 8)\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        return parallel_train(self.model, train_data)\n",
                "    \n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        test_data: RDD = format_rdd(test_data.rdd)\n",
                "        value: RDD = parallel_evaluate(self.model, test_data)\n",
                "        return parallel_binary_cross_entropy(self.model, test_data.map(lambda x: x[0]).zip(value))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SequentialEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = SerialLogisticRegression(self.iterations, self.lr, self.batch_size)\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        pass\n",
                "\n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
                "from pyspark.mllib.regression import LabeledPoint\n",
                "\n",
                "class MLibModelEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = None\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        labels = train_data.rdd.map(lambda x: LabeledPoint(label = x[0], features=x[1:]))        \n",
                "        self.model = LogisticRegressionWithSGD.train(labels, iterations=self.iterations, regParam=self.lr, convergenceTol=0, validateData=False, intercept=True)\n",
                "        return [], []\n",
                "\n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        pass\n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval = ParallelModelEvaluator(100, 0.01, 0.1, 0)\n",
                "eval.train(folds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.4 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "5012df37ab392b9f7e40bd0f27b82c3885414c9b5e6b3ef97e27ea44f17725dd"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
