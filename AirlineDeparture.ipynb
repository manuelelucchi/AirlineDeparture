{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from kaggle.api.kaggle_api_extended import KaggleApi\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pyspark.sql.types import StructField, StringType, StructType\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import monotonically_increasing_id\n",
                "import matplotlib.pyplot as plt\n",
                "import time as tm\n",
                "import itertools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "usePyspark = False\n",
                "path = './data'\n",
                "worker_nodes = \"*\"\n",
                "problem_to_solve = 'CANCELLED'\n",
                "\n",
                "dataset_limit = 10000\n",
                "use_all_dataset_frames = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DA SCRIVERE\n",
                "- perche' usiamo i dataframe invece degli rdd\n",
                "- aggiungere k fold cross validation\n",
                "- aggiungere griglia parametri\n",
                "- aggiungere label stratification\n",
                "- aggiungere performance modello pyspark\n",
                "- aggiungere check e info extra su dataset di base (es sbilanciamento)\n",
                "- auroc, auprc, f1, \n",
                "- confronto con tree classifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['KAGGLE_USERNAME'] = \"davidetricella\"\n",
                "os.environ['KAGGLE_KEY'] = \"e1ab3aae4a07f36b37a3a8bace74d9df\"\n",
                "\n",
                "\n",
                "dataset = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
                "path = './data'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_dataset():\n",
                "    if not os.path.isdir(path):\n",
                "        os.mkdir(path)\n",
                "    if not os.listdir(path):\n",
                "        try:\n",
                "            api = KaggleApi()\n",
                "            api.authenticate()\n",
                "            api.dataset_download_files(dataset, path, unzip=True, quiet=False)\n",
                "        except:\n",
                "            print(\"Error downloading the dataset\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataframe_schema = StructType([\n",
                "    StructField('FL_DATE', StringType(), True),\n",
                "    StructField('OP_CARRIER', StringType(), True),\n",
                "    StructField('ORIGIN', StringType(), True),\n",
                "    StructField('DEST', StringType(), True),\n",
                "    StructField('CRS_DEP_TIME', StringType(), True),\n",
                "    StructField('CRS_ARR_TIME', StringType(), True),\n",
                "    StructField('CANCELLED', StringType(), True),\n",
                "    StructField('DIVERTED', StringType(), True),\n",
                "    StructField('CRS_ELAPSED_TIME', StringType(), True),\n",
                "    StructField('DISTANCE', StringType(), True)\n",
                "])\n",
                "\n",
                "columns_to_get = [\n",
                "    'FL_DATE',\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "\n",
                "if usePyspark:\n",
                "    spark = SparkSession.builder \\\n",
                "    .appName(\"Airline Departure\") \\\n",
                "    .master('local[' + worker_nodes + ']') \\\n",
                "    .getOrCreate()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(usePyspark: bool):\n",
                "    if usePyspark:\n",
                "        data = spark.read.format(\"csv\") \\\n",
                "            .option(\"header\", True) \\\n",
                "            .load(path + '/preprocessed')\n",
                "    else:\n",
                "        data = pd.read_csv(filepath_or_buffer=path + '/' + 'preprocessed.csv')\n",
                "\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def save_dataset(data, usePyspark: bool):\n",
                "    if usePyspark:\n",
                "        data.write.format('csv').option('header', True).mode('overwrite').option(\n",
                "            'sep', ',').save(path + '/preprocessed')\n",
                "    else:\n",
                "        data.to_csv(path_or_buf=path + '/' + 'preprocessed.csv', index=False)\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def check_preprocessed_data_exists() -> bool:\n",
                "    files = os.listdir('./data')\n",
                "    for f in files:\n",
                "        if f.startswith('preprocessed'):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def get_dataset(limit: float = -1, allFrames: bool = True, usePyspark: bool = False):\n",
                "    files = os.listdir(path)\n",
                "    if usePyspark:\n",
                "        big_frame = spark.createDataFrame(\n",
                "            spark.sparkContext.emptyRDD(), schema=dataframe_schema)\n",
                "    else:\n",
                "        big_frame = pd.DataFrame()\n",
                "\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            if usePyspark:\n",
                "                frame = spark.read.option(\"header\", True).csv(path + '/' + f)\n",
                "                frame = frame.select(columns_to_get)\n",
                "                frame = frame.sample(fraction=1.0, withReplacement=False)\n",
                "\n",
                "                if limit != -1:\n",
                "                    frame = frame.limit(limit)\n",
                "\n",
                "                big_frame = frame.union(big_frame)\n",
                "            else:\n",
                "                frame = pd.read_csv(filepath_or_buffer=path +\n",
                "                                    '/' + f, usecols=columns_to_get)\n",
                "                if limit != -1:\n",
                "                    frame = frame.sample(n=limit, replace=False)\n",
                "                big_frame = pd.concat([big_frame, frame])\n",
                "\n",
                "    if usePyspark:\n",
                "        big_frame = big_frame.select(\n",
                "            \"*\").withColumn(\"index\", monotonically_increasing_id())\n",
                "        big_frame.count()\n",
                "\n",
                "    return big_frame\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "default_values = {\n",
                "    'CANCELLED': 0,\n",
                "    'DIVERTED': 0\n",
                "}\n",
                "\n",
                "columns_to_remove_for_canceled = [\n",
                "    'DIVERTED',  # the flight has been diverted to an unplanned airport\n",
                "]\n",
                "\n",
                "columns_to_remove_for_diverted = [\n",
                "    'CANCELLED',  # the flight has been cancelled\n",
                "]\n",
                "\n",
                "names_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "]\n",
                "\n",
                "date_columns_to_convert = [\n",
                "    'FL_DATE'\n",
                "]\n",
                "\n",
                "time_columns_to_convert = [\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME'\n",
                "]\n",
                "\n",
                "numeric_columns_to_convert = [\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "string_columns_to_convert = [\n",
                "    'CANCELLED',\n",
                "    'DIVERTED'\n",
                "]\n",
                "\n",
                "preprocess_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'FL_DATE',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'index'\n",
                "]\n",
                "\n",
                "max_distance = 4970"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "time_file = open(\"./data/times.txt\", \"a\")\n",
                "\n",
                "def print_and_save_time(s: str):\n",
                "    time_file.write(s + '\\n')\n",
                "    print(s)\n",
                "\n",
                "\n",
                "def common_preprocess(data: ps.DataFrame | pd.DataFrame, usePyspark: bool) -> ps.DataFrame | pd.DataFrame:\n",
                "\n",
                "    common_start_time = tm.time()\n",
                "\n",
                "    if usePyspark:\n",
                "        # Replace Nan values with the correct default values\n",
                "        data = data.fillna(value=0)\n",
                "        # Remove rows with Nan key values\n",
                "        data = data.dropna(how='any')\n",
                "    else:\n",
                "        data.fillna(value=0, inplace=True)\n",
                "        data.dropna(how='any', axis='index', inplace=True)\n",
                "\n",
                "    null_removal_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Null values removal concluded: \" +\n",
                "                        str(null_removal_finish_time) + \" seconds\")\n",
                "\n",
                "    names_start_time = tm.time()\n",
                "    data = convert_names_into_numbers(data, usePyspark)\n",
                "    names_finish_time = tm.time() - names_start_time\n",
                "    print_and_save_time(\"Names conversion concluded: \" +\n",
                "                        str(names_finish_time) + \" seconds\")\n",
                "\n",
                "    dates_start_time = tm.time()\n",
                "    data = convert_dates_into_numbers(data, usePyspark)\n",
                "    dates_finish_time = tm.time() - dates_start_time\n",
                "    print_and_save_time(\"Dates conversion concluded: \" +\n",
                "                        str(dates_finish_time) + \" seconds\")\n",
                "\n",
                "    times_start_time = tm.time()\n",
                "    data = convert_times_into_numbers(data, usePyspark)\n",
                "    times_finish_time = tm.time() - times_start_time\n",
                "    print_and_save_time(\"Times conversion concluded: \" +\n",
                "                        str(times_finish_time) + \" seconds\")\n",
                "\n",
                "    distance_start_time = tm.time()\n",
                "    data = convert_distance_into_numbers(data, usePyspark)\n",
                "    distance_finish_time = tm.time() - distance_start_time\n",
                "    print_and_save_time(\"Distance conversion concluded: \" +\n",
                "                        str(distance_finish_time) + \" seconds\")\n",
                "\n",
                "    if usePyspark:\n",
                "        strings_start_time = tm.time()\n",
                "        data = convert_strings_into_numbers(data)\n",
                "        strings_finish_time = tm.time() - strings_start_time\n",
                "        print_and_save_time(\"Strings conversion concluded: \" +\n",
                "                            str(strings_finish_time) + \" seconds\")\n",
                "\n",
                "    common_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Common preprocessing concluded: \" +\n",
                "                        str(common_finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "if not check_preprocessed_data_exists():\n",
                "        download_dataset()\n",
                "\n",
                "        start_time = tm.time()\n",
                "        data = get_dataset(dataset_limit, use_all_dataset_frames, usePyspark)\n",
                "\n",
                "        finish_time = tm.time() - start_time\n",
                "        print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                            str(finish_time) + \" seconds\")\n",
                "\n",
                "        data = common_preprocess(data, usePyspark)\n",
                "        read.save_dataset(data, usePyspark)\n",
                "else:\n",
                "    data = read.load_dataset(usePyspark)\n",
                "    if usePyspark:\n",
                "        udf_string_conversion = udf(lambda x: float(x), DoubleType())\n",
                "        for c in preprocess_columns_to_convert:\n",
                "            data = data.withColumn(c, udf_string_conversion(col(c)))\n",
                "    \n",
                "data = remove_extra_columns(index, data, usePyspark)\n",
                "\n",
                "start_time = tm.time()\n",
                "preprocessing_splits = split_data(data, usePyspark, index, split_number)\n",
                "\n",
                "if usePyspark:\n",
                "\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "else:\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "\n",
                "# Preprocess\n",
                "\n",
                "## Da aggiungere Z score normalization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generic Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    '''\n",
                "    Calculates the sigmoid of the given data\n",
                "    '''\n",
                "    g = 1.0 / (1.0 + np.exp(-x))\n",
                "    return g\n",
                "\n",
                "def binary_cross_entropy(y, y_label, w, l2):\n",
                "    '''\n",
                "    Calculates the binary cross entropy loss of the calculated y and the given y_label\n",
                "    '''\n",
                "    loss = -np.mean(y_label*(np.log(y)) + (1-y_label)\n",
                "                    * np.log(1-y)) + regularize(w, l2)\n",
                "    return loss\n",
                "\n",
                "def regularize(W, l2):\n",
                "    '''\n",
                "    Calculates the regularization term for the loss\n",
                "    '''\n",
                "    return (l2 / 2) * np.sum(np.square(W))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Numpy Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LogisticRegression():\n",
                "    def __init__(self, learning_rate: float, batch_size: int, l2: float):\n",
                "        self.learning_rate = learning_rate\n",
                "        self.batch_size = batch_size\n",
                "        self.l2 = l2\n",
                "\n",
                "    def initialize(self, columns_number):\n",
                "        self.W = np.random.rand(columns_number)\n",
                "        self.b = np.random.rand()\n",
                "\n",
                "    def evaluate(self, X):\n",
                "        Z = np.dot(X, self.W) + self.b\n",
                "        Z = sigmoid(Z)\n",
                "        return Z\n",
                "\n",
                "    def gradient(self, X, Y, Y_label):\n",
                "        '''\n",
                "        Calculates the gradient w.r.t weights and bias\n",
                "        '''\n",
                "\n",
                "        # Number of training examples.\n",
                "        m = X.shape[0]\n",
                "\n",
                "        # Gradient of loss w.r.t weights with regularization\n",
                "        dw = (1/m)*np.dot(X.T, (Y - Y_label)) + self.l2 * self.W\n",
                "\n",
                "        # Gradient of loss w.r.t bias with regularization\n",
                "        db = (1/m)*np.sum((Y - Y_label))\n",
                "\n",
                "        return dw, db\n",
                "\n",
                "    def update(self, dW, db):\n",
                "        self.W = self.W - self.learning_rate * dW\n",
                "        self.b = self.b - self.learning_rate * db\n",
                "\n",
                "    def train(self, X, Y_labels, iterations = 10):\n",
                "        self.initialize(X.shape[1])\n",
                "        losses = []\n",
                "        gradients = []\n",
                "\n",
                "        for _ in range(iterations):\n",
                "            _losses = []\n",
                "            _gradients = []\n",
                "            for b in range(X.shape[0]//self.batch_size):\n",
                "                b_X = X[b*self.batch_size:b*self.batch_size+self.batch_size, :]\n",
                "                b_Y_labels = Y_labels[b*self.batch_size:b *\n",
                "                                      self.batch_size+self.batch_size]\n",
                "                Y = self.evaluate(b_X)\n",
                "                _losses.append(binary_cross_entropy(\n",
                "                    Y, b_Y_labels, self.W, self.l2))\n",
                "                (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "                _gradients.append(dW)\n",
                "                self.update(dW, db)\n",
                "            losses.append(np.mean(_losses))\n",
                "            gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pyspak Model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiments\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_roc(labels, results, name):\n",
                "    labels_and_results = sorted(\n",
                "        list(zip(labels, map(lambda x: x, results))), key=lambda x: x[1])\n",
                "\n",
                "    labels_by_weights = np.array([k for (k, _) in labels_and_results])\n",
                "\n",
                "    length = labels_by_weights.size\n",
                "\n",
                "    true_positives = labels_by_weights.cumsum()\n",
                "\n",
                "    num_positive = true_positives[-1]\n",
                "\n",
                "    false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
                "\n",
                "    true_positives_rate = true_positives / num_positive\n",
                "    false_positives_rate = false_positives / (length - num_positive)\n",
                "\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
                "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
                "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
                "    plt.plot(false_positives_rate, true_positives_rate,\n",
                "             color='#8cbfd0', linestyle='-', linewidth=3.)\n",
                "    plt.plot((0., 1.), (0., 1.), linestyle='--',\n",
                "             color='#d6ebf2', linewidth=2.)\n",
                "\n",
                "    plt.savefig('./data/{}_roc.png'.format(name))\n",
                "    fig.clear()\n",
                "    plt.close()\n",
                "\n",
                "def plot_loss_gradient(iterations, train_losses, gradients, name):\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlabel('Iterations')\n",
                "    ax.set_ylabel('Loss/Gradient')\n",
                "    ax.set_title(name)\n",
                "    ax.plot(range(iterations), train_losses, label='Loss')\n",
                "    ax.plot(range(iterations), gradients, label='Gradient')\n",
                "    ax.grid()\n",
                "    ax.legend()\n",
                "\n",
                "    fig.savefig(\"./data/{}.png\".format(name))\n",
                "    fig.clear()\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hyperparamters Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(100, 0.001, 0),\n",
                            " (100, 0.001, 0.1),\n",
                            " (100, 0.001, 0.001),\n",
                            " (100, 0.01, 0),\n",
                            " (100, 0.01, 0.1),\n",
                            " (100, 0.01, 0.001),\n",
                            " (100, 0.1, 0),\n",
                            " (100, 0.1, 0.1),\n",
                            " (100, 0.1, 0.001),\n",
                            " (200, 0.001, 0),\n",
                            " (200, 0.001, 0.1),\n",
                            " (200, 0.001, 0.001),\n",
                            " (200, 0.01, 0),\n",
                            " (200, 0.01, 0.1),\n",
                            " (200, 0.01, 0.001),\n",
                            " (200, 0.1, 0),\n",
                            " (200, 0.1, 0.1),\n",
                            " (200, 0.1, 0.001),\n",
                            " (500, 0.001, 0),\n",
                            " (500, 0.001, 0.1),\n",
                            " (500, 0.001, 0.001),\n",
                            " (500, 0.01, 0),\n",
                            " (500, 0.01, 0.1),\n",
                            " (500, 0.01, 0.001),\n",
                            " (500, 0.1, 0),\n",
                            " (500, 0.1, 0.1),\n",
                            " (500, 0.1, 0.001)]"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grid = { 'iter': [100, 200, 500], 'lr': [0.001, 0.01, 0.1], 'l2': [0, 0.1, 0.001]}\n",
                "\n",
                "params = list(itertools.product(*grid.values()))\n",
                "\n",
                "params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Evaluator:\n",
                "    def __init__(self, params):\n",
                "        pass\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.4 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "5012df37ab392b9f7e40bd0f27b82c3885414c9b5e6b3ef97e27ea44f17725dd"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}