{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructField, StringType, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usePyspark = False\n",
    "path = './data'\n",
    "worker_nodes = \"*\"\n",
    "problem_to_solve = 'CANCELLED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"davidetricella\"\n",
    "os.environ['KAGGLE_KEY'] = \"e1ab3aae4a07f36b37a3a8bace74d9df\"\n",
    "\n",
    "\n",
    "dataset = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
    "path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    if not os.listdir(path):\n",
    "        try:\n",
    "            api = KaggleApi()\n",
    "            api.authenticate()\n",
    "            api.dataset_download_files(dataset, path, unzip=True, quiet=False)\n",
    "        except:\n",
    "            print(\"Error downloading the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_schema = StructType([\n",
    "    StructField('FL_DATE', StringType(), True),\n",
    "    StructField('OP_CARRIER', StringType(), True),\n",
    "    StructField('ORIGIN', StringType(), True),\n",
    "    StructField('DEST', StringType(), True),\n",
    "    StructField('CRS_DEP_TIME', StringType(), True),\n",
    "    StructField('CRS_ARR_TIME', StringType(), True),\n",
    "    StructField('CANCELLED', StringType(), True),\n",
    "    StructField('DIVERTED', StringType(), True),\n",
    "    StructField('CRS_ELAPSED_TIME', StringType(), True),\n",
    "    StructField('DISTANCE', StringType(), True)\n",
    "])\n",
    "\n",
    "columns_to_get = [\n",
    "    'FL_DATE',\n",
    "    'OP_CARRIER',\n",
    "    'ORIGIN',\n",
    "    'DEST',\n",
    "    'CRS_DEP_TIME',\n",
    "    'CRS_ARR_TIME',\n",
    "    'CANCELLED',\n",
    "    'DIVERTED',\n",
    "    'CRS_ELAPSED_TIME',\n",
    "    'DISTANCE'\n",
    "]\n",
    "\n",
    "\n",
    "if usePyspark:\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Airline Departure\") \\\n",
    "    .master('local[' + worker_nodes + ']') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(usePyspark: bool):\n",
    "    if usePyspark:\n",
    "        data = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .load(path + '/preprocessed')\n",
    "    else:\n",
    "        data = pd.read_csv(filepath_or_buffer=path + '/' + 'preprocessed.csv')\n",
    "\n",
    "    print('Preprocessed dataset loaded')\n",
    "    return data\n",
    "\n",
    "def save_dataset(data, usePyspark: bool):\n",
    "    if usePyspark:\n",
    "        data.write.format('csv').option('header', True).mode('overwrite').option(\n",
    "            'sep', ',').save(path + '/preprocessed')\n",
    "    else:\n",
    "        data.to_csv(path_or_buf=path + '/' + 'preprocessed.csv', index=False)\n",
    "    print('Preprocessed dataset saved')\n",
    "\n",
    "def check_preprocessed_data_exists() -> bool:\n",
    "    files = os.listdir('./data')\n",
    "    for f in files:\n",
    "        if f.startswith('preprocessed'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_dataset(limit: float = -1, allFrames: bool = True, usePyspark: bool = False):\n",
    "    files = os.listdir(path)\n",
    "    if usePyspark:\n",
    "        big_frame = spark.createDataFrame(\n",
    "            spark.sparkContext.emptyRDD(), schema=dataframe_schema)\n",
    "    else:\n",
    "        big_frame = pd.DataFrame()\n",
    "\n",
    "    if not allFrames:\n",
    "        files = [files[0]]\n",
    "\n",
    "    for f in files:\n",
    "        if f.endswith('.csv'):\n",
    "            if usePyspark:\n",
    "                frame = spark.read.option(\"header\", True).csv(path + '/' + f)\n",
    "                frame = frame.select(columns_to_get)\n",
    "                frame = frame.sample(fraction=1.0, withReplacement=False)\n",
    "\n",
    "                if limit != -1:\n",
    "                    frame = frame.limit(limit)\n",
    "\n",
    "                big_frame = frame.union(big_frame)\n",
    "            else:\n",
    "                frame = pd.read_csv(filepath_or_buffer=path +\n",
    "                                    '/' + f, usecols=columns_to_get)\n",
    "                if limit != -1:\n",
    "                    frame = frame.sample(n=limit, replace=False)\n",
    "                big_frame = pd.concat([big_frame, frame])\n",
    "\n",
    "    if usePyspark:\n",
    "        big_frame = big_frame.select(\n",
    "            \"*\").withColumn(\"index\", monotonically_increasing_id())\n",
    "        big_frame.count()\n",
    "\n",
    "    return big_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_values = {\n",
    "    'CANCELLED': 0,\n",
    "    'DIVERTED': 0\n",
    "}\n",
    "\n",
    "columns_to_remove_for_canceled = [\n",
    "    'DIVERTED',  # the flight has been diverted to an unplanned airport\n",
    "]\n",
    "\n",
    "columns_to_remove_for_diverted = [\n",
    "    'CANCELLED',  # the flight has been cancelled\n",
    "]\n",
    "\n",
    "names_columns_to_convert = [\n",
    "    'OP_CARRIER',\n",
    "    'ORIGIN',\n",
    "    'DEST',\n",
    "]\n",
    "\n",
    "date_columns_to_convert = [\n",
    "    'FL_DATE'\n",
    "]\n",
    "\n",
    "time_columns_to_convert = [\n",
    "    'CRS_DEP_TIME',\n",
    "    'CRS_ARR_TIME',\n",
    "    'CRS_ELAPSED_TIME'\n",
    "]\n",
    "\n",
    "numeric_columns_to_convert = [\n",
    "    'DISTANCE'\n",
    "]\n",
    "\n",
    "string_columns_to_convert = [\n",
    "    'CANCELLED',\n",
    "    'DIVERTED'\n",
    "]\n",
    "\n",
    "preprocess_columns_to_convert = [\n",
    "    'OP_CARRIER',\n",
    "    'ORIGIN',\n",
    "    'DEST',\n",
    "    'FL_DATE',\n",
    "    'CRS_DEP_TIME',\n",
    "    'CRS_ARR_TIME',\n",
    "    'CRS_ELAPSED_TIME',\n",
    "    'DISTANCE',\n",
    "    'CANCELLED',\n",
    "    'DIVERTED',\n",
    "    'index'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distance = 4970\n",
    "\n",
    "# Preprocess\n",
    "\n",
    "## Da aggiungere Z score normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Calculates the sigmoid of the given data\n",
    "    '''\n",
    "    g = 1.0 / (1.0 + np.exp(-x))\n",
    "    return g\n",
    "\n",
    "def binary_cross_entropy(y, y_label, w, l2):\n",
    "    '''\n",
    "    Calculates the binary cross entropy loss of the calculated y and the given y_label\n",
    "    '''\n",
    "    loss = -np.mean(y_label*(np.log(y)) + (1-y_label)\n",
    "                    * np.log(1-y)) + regularize(w, l2)\n",
    "    return loss\n",
    "\n",
    "def regularize(W, l2):\n",
    "    '''\n",
    "    Calculates the regularization term for the loss\n",
    "    '''\n",
    "    return (l2 / 2) * np.sum(np.square(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, learning_rate: float, batch_size: int, l2: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.l2 = l2\n",
    "\n",
    "    def initialize(self, columns_number):\n",
    "        self.W = np.random.rand(columns_number)\n",
    "        self.b = np.random.rand()\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        Z = sigmoid(Z)\n",
    "        return Z\n",
    "\n",
    "    def gradient(self, X, Y, Y_label):\n",
    "        '''\n",
    "        Calculates the gradient w.r.t weights and bias\n",
    "        '''\n",
    "\n",
    "        # Number of training examples.\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Gradient of loss w.r.t weights with regularization\n",
    "        dw = (1/m)*np.dot(X.T, (Y - Y_label)) + self.l2 * self.W\n",
    "\n",
    "        # Gradient of loss w.r.t bias with regularization\n",
    "        db = (1/m)*np.sum((Y - Y_label))\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "    def update(self, dW, db):\n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "\n",
    "    def train(self, X, Y_labels, iterations = 10):\n",
    "        self.initialize(X.shape[1])\n",
    "        losses = []\n",
    "        gradients = []\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            _losses = []\n",
    "            _gradients = []\n",
    "            for b in range(X.shape[0]//self.batch_size):\n",
    "                b_X = X[b*self.batch_size:b*self.batch_size+self.batch_size, :]\n",
    "                b_Y_labels = Y_labels[b*self.batch_size:b *\n",
    "                                      self.batch_size+self.batch_size]\n",
    "                Y = self.evaluate(b_X)\n",
    "                _losses.append(binary_cross_entropy(\n",
    "                    Y, b_Y_labels, self.W, self.l2))\n",
    "                (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
    "                _gradients.append(dW)\n",
    "                self.update(dW, db)\n",
    "            losses.append(np.mean(_losses))\n",
    "            gradients.append(np.mean(_gradients))\n",
    "\n",
    "        return (losses, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspak Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_roc(labels, results, name):\n",
    "    labels_and_results = sorted(\n",
    "        list(zip(labels, map(lambda x: x, results))), key=lambda x: x[1])\n",
    "\n",
    "    labels_by_weights = np.array([k for (k, _) in labels_and_results])\n",
    "\n",
    "    length = labels_by_weights.size\n",
    "\n",
    "    true_positives = labels_by_weights.cumsum()\n",
    "\n",
    "    num_positive = true_positives[-1]\n",
    "\n",
    "    false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
    "\n",
    "    true_positives_rate = true_positives / num_positive\n",
    "    false_positives_rate = false_positives / (length - num_positive)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.plot(false_positives_rate, true_positives_rate,\n",
    "             color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "    plt.plot((0., 1.), (0., 1.), linestyle='--',\n",
    "             color='#d6ebf2', linewidth=2.)\n",
    "\n",
    "    plt.savefig('./data/{}_roc.png'.format(name))\n",
    "    fig.clear()\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_gradient(iterations, train_losses, gradients, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Loss/Gradient')\n",
    "    ax.set_title(name)\n",
    "    ax.plot(range(iterations), train_losses, label='Loss')\n",
    "    ax.plot(range(iterations), gradients, label='Gradient')\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "    fig.savefig(\"./data/{}.png\".format(name))\n",
    "    fig.clear()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e51fdbc37c5adaa12a9ed97b50fdaf0ff46be7ee1a114cc405b6eaf0c36d1bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
