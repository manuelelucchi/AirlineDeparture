{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from kaggle.api.kaggle_api_extended import KaggleApi\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import monotonically_increasing_id, col, udf, rand\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "import pyspark.sql as ps\n",
                "from zlib import crc32\n",
                "import time as tm\n",
                "from datetime import datetime as dt\n",
                "import itertools\n",
                "from dataclasses import dataclass\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.rdd import RDD\n",
                "from pyspark.broadcast import Broadcast\n",
                "import findspark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = './data'\n",
                "worker_nodes = \"*\"\n",
                "problem_to_solve = 'CANCELLED'\n",
                "\n",
                "dataset_limit = 10000\n",
                "use_all_dataset_frames = True\n",
                "number_of_splits = 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DA SCRIVERE\n",
                "- perche' usiamo i dataframe invece degli rdd nella prima parte\n",
                "- aggiungere k fold cross validation\n",
                "- aggiungere griglia parametri\n",
                "- aggiungere label stratification\n",
                "- aggiungere performance modello pyspark\n",
                "- aggiungere check e info extra su dataset di base (es sbilanciamento)\n",
                "- auroc, auprc, f1, \n",
                "- confronto con tree classifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['KAGGLE_USERNAME'] = \"davidetricella\"\n",
                "os.environ['KAGGLE_KEY'] = \"e1ab3aae4a07f36b37a3a8bace74d9df\"\n",
                "\n",
                "\n",
                "dataset = 'yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018'\n",
                "path = './data'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_dataset():\n",
                "    if not os.path.isdir(path):\n",
                "        os.mkdir(path)\n",
                "    if not os.listdir(path):\n",
                "        try:\n",
                "            api = KaggleApi()\n",
                "            api.authenticate()\n",
                "            api.dataset_download_files(dataset, path, unzip=True, quiet=False)\n",
                "        except:\n",
                "            print(\"Error downloading the dataset\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataframe_schema = StructType([\n",
                "    StructField('FL_DATE', StringType(), True),\n",
                "    StructField('OP_CARRIER', StringType(), True),\n",
                "    StructField('ORIGIN', StringType(), True),\n",
                "    StructField('DEST', StringType(), True),\n",
                "    StructField('CRS_DEP_TIME', StringType(), True),\n",
                "    StructField('CRS_ARR_TIME', StringType(), True),\n",
                "    StructField('CANCELLED', StringType(), True),\n",
                "    StructField('DIVERTED', StringType(), True),\n",
                "    StructField('CRS_ELAPSED_TIME', StringType(), True),\n",
                "    StructField('DISTANCE', StringType(), True)\n",
                "])\n",
                "\n",
                "columns_to_get = [\n",
                "    'FL_DATE',\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "findspark.init()\n",
                "findspark.find()\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                ".appName(\"Airline Departure\") \\\n",
                ".master('local[' + worker_nodes + ']') \\\n",
                ".getOrCreate()\n",
                "\n",
                "context = spark.sparkContext"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset():\n",
                "    data = spark.read.format(\"csv\") \\\n",
                "        .option(\"header\", True) \\\n",
                "        .load(path + '/preprocessed')\n",
                "\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def save_dataset(data):\n",
                "    data.write.format('csv').option('header', True).mode('overwrite').option(\n",
                "        'sep', ',').save(path + '/preprocessed')\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def check_preprocessed_data_exists() -> bool:\n",
                "    files = os.listdir('./data')\n",
                "    for f in files:\n",
                "        if f.startswith('preprocessed'):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def get_dataset(limit: float = -1, allFrames: bool = True):\n",
                "    files = os.listdir(path)\n",
                "    big_frame = spark.createDataFrame(\n",
                "        spark.sparkContext.emptyRDD(), schema=dataframe_schema)\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            frame = spark.read.option(\"header\", True).csv(path + '/' + f)\n",
                "            frame = frame.select(columns_to_get)\n",
                "            frame = frame.orderBy(rand())\n",
                "\n",
                "            if limit != -1:\n",
                "                frame = frame.limit(limit)\n",
                "\n",
                "            big_frame = frame.union(big_frame)\n",
                "\n",
                "    big_frame = big_frame.select(\n",
                "        \"*\").withColumn(\"index\", monotonically_increasing_id())\n",
                "    big_frame.count()\n",
                "\n",
                "    return big_frame\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "default_values = {\n",
                "    'CANCELLED': 0,\n",
                "    'DIVERTED': 0\n",
                "}\n",
                "\n",
                "columns_to_remove_for_canceled = [\n",
                "    'DIVERTED',  # the flight has been diverted to an unplanned airport\n",
                "]\n",
                "\n",
                "columns_to_remove_for_diverted = [\n",
                "    'CANCELLED',  # the flight has been cancelled\n",
                "]\n",
                "\n",
                "names_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "]\n",
                "\n",
                "date_columns_to_convert = [\n",
                "    'FL_DATE'\n",
                "]\n",
                "\n",
                "time_columns_to_convert = [\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME'\n",
                "]\n",
                "\n",
                "numeric_columns_to_convert = [\n",
                "    'DISTANCE'\n",
                "]\n",
                "\n",
                "string_columns_to_convert = [\n",
                "    'CANCELLED',\n",
                "    'DIVERTED'\n",
                "]\n",
                "\n",
                "preprocess_columns_to_convert = [\n",
                "    'OP_CARRIER',\n",
                "    'ORIGIN',\n",
                "    'DEST',\n",
                "    'FL_DATE',\n",
                "    'CRS_DEP_TIME',\n",
                "    'CRS_ARR_TIME',\n",
                "    'CRS_ELAPSED_TIME',\n",
                "    'DISTANCE',\n",
                "    'CANCELLED',\n",
                "    'DIVERTED',\n",
                "    'index'\n",
                "]\n",
                "\n",
                "max_distance = 4970"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#CHARTS PLOTTING\n",
                "def plot_balancing_chart(data: ps.DataFrame, label: str):\n",
                "  total_positives = data.filter(col(label) == 1).count()\n",
                "  total_negatives = data.filter(col(label) == 0).count()\n",
                "  fig, ax = plt.subplots()\n",
                "\n",
                "  labels = ['REGULAR', label]\n",
                "  counts = [total_negatives, total_positives]\n",
                "  bar_colors = ['tab:blue', 'tab:red']\n",
                "\n",
                "  ax.bar(labels, counts, color=bar_colors)\n",
                "\n",
                "  ax.set_ylabel('Counts')\n",
                "  ax.set_title('Regular flights and problematic flights counts')\n",
                "\n",
                "  plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_and_save_time(s: str):\n",
                "    time_file.write(s + '\\n')\n",
                "    print(s)\n",
                "\n",
                "\n",
                "def remove_extra_columns(index: str, data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    start_time = tm.time()\n",
                "    oppositeIndex = 'DIVERTED' if index == 'CANCELLED' else 'CANCELLED'\n",
                "    data = data.drop(oppositeIndex)\n",
                "    data = data.drop('index')\n",
                "    data.count()\n",
                "\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Extra column removal concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_strings_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "    udf_string_conversion = udf(lambda x: float(x), DoubleType())\n",
                "    for c in string_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_string_conversion(col(c)))\n",
                "    data.count()\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_names_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    def str_to_float(s: str):\n",
                "        encoding = \"utf-8\"\n",
                "        b = s.encode(encoding)\n",
                "        return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "    udf_names_conversion = udf(lambda x: str_to_float(x), DoubleType())\n",
                "    for c in names_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_names_conversion(col(c)))\n",
                "    data.count()\n",
                "    return data\n",
                "\n",
                "\n",
                "def convert_dates_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "    multiplier: float = 1 / 365\n",
                "\n",
                "    def date_to_day_of_year(date_string) -> float:\n",
                "\n",
                "        date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "        day = date.timetuple().tm_yday - 1\n",
                "        return day * multiplier\n",
                "\n",
                "    udf_dates_conversion = udf(\n",
                "        lambda x: date_to_day_of_year(x), DoubleType())\n",
                "    for c in date_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_dates_conversion(col(c)))\n",
                "    data.count()\n",
                "\n",
                "    return data\n",
                "\n",
                "def balance_dataframe(data: ps.DataFrame, index: str) -> ps.DataFrame:\n",
                "    start_time = tm.time()\n",
                "    irregular_flights = data.filter(col(index) == 1)\n",
                "    regular_flights = data.filter(col(index) == 0)\n",
                "\n",
                "    result = regular_flights.limit(irregular_flights.count()).\\\n",
                "        union(irregular_flights).\\\n",
                "        orderBy(rand())\n",
                "\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Dataset balancing concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return result\n",
                "\n",
                "#DA CONVERTIRE SENZA UDF\n",
                "def convert_times_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    def time_to_interval(time) -> float:\n",
                "        t = int(float(time))\n",
                "        h = t // 100\n",
                "        m = t % 100\n",
                "        t = h * 60 + m\n",
                "        return float(t / 1140)\n",
                "\n",
                "    udf_time_conversion = udf(lambda x: time_to_interval(x), DoubleType())\n",
                "    for c in time_columns_to_convert:\n",
                "        data = data.withColumn(c, udf_time_conversion(col(c)))\n",
                "    data.count()\n",
                "\n",
                "    return data\n",
                "\n",
                "#DA CONVERTIRE SENZA UDF\n",
                "def convert_distance_into_numbers(data: ps.DataFrame) -> ps.DataFrame:\n",
                "\n",
                "    multiplier: float = float(1) / float(max_distance)\n",
                "    udf_numeric_conversion = udf(\n",
                "        lambda x: float(x) * multiplier, DoubleType())\n",
                "\n",
                "    data = data.withColumn(\n",
                "        'DISTANCE', udf_numeric_conversion(col('DISTANCE')))\n",
                "    data.count()\n",
                "    return data\n",
                "\n",
                "\n",
                "def z_score_normalize(data: ps.DataFrame) -> ps.DataFrame:\n",
                "    pass\n",
                "\n",
                "\n",
                "\n",
                "def split_data(data: ps.DataFrame, label: str, k: int) -> tuple[ps.DataFrame, ps.DataFrame]:\n",
                "    split_list = []\n",
                "\n",
                "    data = data.dropDuplicates()\n",
                "\n",
                "    k_elements_half_number = math.floor((data.count() / k) / 2)\n",
                "\n",
                "    i = 0\n",
                "    while i < k:\n",
                "        k_positive_sample = data.where(\n",
                "            col(label) == 1).limit(k_elements_half_number)\n",
                "        k_negative_sample = data.where(\n",
                "            col(label) == 0).limit(k_elements_half_number)\n",
                "        k_sample = k_positive_sample.union(k_negative_sample)\n",
                "\n",
                "        split_list.append(k_sample)\n",
                "        data = data.subtract(k_sample)\n",
                "\n",
                "        print(\"Split \" + str(i + 1) + \" of \" + str(k) + \" completed\")\n",
                "        i += 1\n",
                "\n",
                "    return split_list\n",
                "\n",
                "#else:\n",
                "#    data = load_dataset()\n",
                "#    udf_string_conversion = udf(lambda x: float(x), DoubleType())\n",
                "#    for c in preprocess_columns_to_convert:\n",
                "#        data = data.withColumn(c, udf_string_conversion(col(c)))\n",
                "    \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset reading concluded: 13.62911343574524 seconds\n"
                    ]
                }
            ],
            "source": [
                "#PREPROCESSING EXECUTION\n",
                "time_file = open(\"./data/times.txt\", \"w\")\n",
                "\n",
                "#if not check_preprocessed_data_exists():\n",
                "download_dataset()\n",
                "start_time = tm.time()\n",
                "data = get_dataset(dataset_limit, use_all_dataset_frames)\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Null values removal concluded: 0.026459217071533203 seconds\n"
                    ]
                }
            ],
            "source": [
                "# Remove rows with Nan key values\n",
                "common_start_time = tm.time()\n",
                "\n",
                "data = data.dropna(how='any')\n",
                "null_removal_finish_time = tm.time() - common_start_time\n",
                "print_and_save_time(\"Null values removal concluded: \" +\n",
                "                    str(null_removal_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset balancing concluded: 28.340028285980225 seconds\n"
                    ]
                }
            ],
            "source": [
                "data = balance_dataframe(data, problem_to_solve)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Names conversion concluded: 54.98914456367493 seconds\n"
                    ]
                }
            ],
            "source": [
                "names_start_time = tm.time()\n",
                "data = convert_names_into_numbers(data)\n",
                "names_finish_time = tm.time() - names_start_time\n",
                "print_and_save_time(\"Names conversion concluded: \" +\n",
                "                    str(names_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dates conversion concluded: 52.610299587249756 seconds\n"
                    ]
                }
            ],
            "source": [
                "dates_start_time = tm.time()\n",
                "data = convert_dates_into_numbers(data)\n",
                "dates_finish_time = tm.time() - dates_start_time\n",
                "print_and_save_time(\"Dates conversion concluded: \" +\n",
                "                    str(dates_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Times conversion concluded: 55.34258413314819 seconds\n"
                    ]
                }
            ],
            "source": [
                "times_start_time = tm.time()\n",
                "data = convert_times_into_numbers(data)\n",
                "times_finish_time = tm.time() - times_start_time\n",
                "print_and_save_time(\"Times conversion concluded: \" +\n",
                "                    str(times_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Distance conversion concluded: 55.27087140083313 seconds\n"
                    ]
                }
            ],
            "source": [
                "distance_start_time = tm.time()\n",
                "data = convert_distance_into_numbers(data)\n",
                "distance_finish_time = tm.time() - distance_start_time\n",
                "print_and_save_time(\"Distance conversion concluded: \" +\n",
                "                    str(distance_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Strings conversion concluded: 57.37093949317932 seconds\n"
                    ]
                }
            ],
            "source": [
                "strings_start_time = tm.time()\n",
                "data = convert_strings_into_numbers(data)\n",
                "strings_finish_time = tm.time() - strings_start_time\n",
                "print_and_save_time(\"Strings conversion concluded: \" +\n",
                "                    str(strings_finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o435.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 436.0 failed 1 times, most recent failure: Lost task 0.0 in stage 436.0 (TID 2432) (192.168.1.89 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\r\n\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m save_dataset(data)\n",
                        "Cell \u001b[1;32mIn [7], line 11\u001b[0m, in \u001b[0;36msave_dataset\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_dataset\u001b[39m(data):\n\u001b[0;32m     10\u001b[0m     data\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moption(\n\u001b[1;32m---> 11\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39msep\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/preprocessed\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPreprocessed dataset saved\u001b[39m\u001b[39m'\u001b[39m)\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
                        "File \u001b[1;32mc:\\Users\\manue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o435.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 436.0 failed 1 times, most recent failure: Lost task 0.0 in stage 436.0 (TID 2432) (192.168.1.89 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\r\n\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\r\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n"
                    ]
                }
            ],
            "source": [
                "save_dataset(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extra column removal concluded: 52.0863401889801 seconds\n"
                    ]
                }
            ],
            "source": [
                "data = remove_extra_columns(problem_to_solve, data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "plot_balancing_chart(data, problem_to_solve)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+--------------------+\n",
                        "|          OP_CARRIER|\n",
                        "+--------------------+\n",
                        "| 0.35488015157170594|\n",
                        "|  0.8907070874702185|\n",
                        "| 0.29755657189525664|\n",
                        "|   0.530499154701829|\n",
                        "|  0.6213391446508467|\n",
                        "|  0.6588134847115725|\n",
                        "| 0.29648267361335456|\n",
                        "| 0.46551953721791506|\n",
                        "|  0.3311674117576331|\n",
                        "|  0.8138902904465795|\n",
                        "|0.009918806841596961|\n",
                        "|  0.6213391446508467|\n",
                        "|  0.3311674117576331|\n",
                        "|  0.6188511815853417|\n",
                        "| 0.29648267361335456|\n",
                        "|  0.6213391446508467|\n",
                        "|  0.6797474827617407|\n",
                        "| 0.29755657189525664|\n",
                        "|   0.699212800944224|\n",
                        "| 0.33370711258612573|\n",
                        "+--------------------+\n",
                        "only showing top 20 rows\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "data.select(F.col('OP_CARRIER')).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Z Score Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "OP_CARRIER\n"
                    ]
                }
            ],
            "source": [
                "column_list = data.columns\n",
                "column_mean_dict = dict()\n",
                "column_stddv_dict = dict()\n",
                "\n",
                "for c in column_list:\n",
                "    column_mean_dict[c] = data.select(F.mean(F.col(c))).head()[0]\n",
                "    column_stddv_dict[c] = data.select(F.stddev(F.col(c))).head()[0]\n",
                "\n",
                "data = data.select(\n",
                "    problem_to_solve,\n",
                "\n",
                "    ((data.OP_CARRIER - column_mean_dict[\"OP_CARRIER\"]) / column_stddv_dict[\"OP_CARRIER\"]).alias('OP_CARRIER'),\n",
                "\n",
                "    ((data.ORIGIN - column_mean_dict[\"ORIGIN\"]) / column_stddv_dict[\"ORIGIN\"]).alias('ORIGIN'),\n",
                "\n",
                "    ((data.DEST - column_mean_dict[\"DEST\"]) / column_stddv_dict[\"DEST\"]).alias('DEST'),\n",
                "\n",
                "    ((data.FL_DATE - column_mean_dict[\"FL_DATE\"]) / column_stddv_dict[\"FL_DATE\"]).alias('FL_DATE'),\n",
                "\n",
                "    ((data.CRS_DEP_TIME - column_mean_dict[\"CRS_DEP_TIME\"]) / column_stddv_dict[\"CRS_DEP_TIME\"]).alias('CRS_DEP_TIME'),\n",
                "\n",
                "    ((data.CRS_ARR_TIME - column_mean_dict[\"CRS_ARR_TIME\"]) /  column_stddv_dict[\"CRS_ARR_TIME\"]).alias('CRS_ARR_TIME'),\n",
                "\n",
                "    ((data.CRS_ELAPSED_TIME - column_mean_dict[\"CRS_ELAPSED_TIME\"]) / column_stddv_dict[\"CRS_ELAPSED_TIME\"]).alias('CRS_ELAPSED_TIME'),\n",
                "\n",
                "    ((data.DISTANCE - column_mean_dict[\"DISTANCE\"]) / column_stddv_dict[\"DISTANCE\"]).alias('DISTANCE')\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_time = tm.time()\n",
                "preprocessing_splits = split_data(data, problem_to_solve, number_of_splits)\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bonus: Pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pandas_load_dataset():\n",
                "    data = pd.read_csv(filepath_or_buffer=path + '/' + 'preprocessed.csv')\n",
                "    print('Preprocessed dataset loaded')\n",
                "    return data\n",
                "\n",
                "def pandas_save_dataset(data):\n",
                "    data.to_csv(path_or_buf=path + '/' + 'preprocessed.csv', index=False)\n",
                "    print('Preprocessed dataset saved')\n",
                "\n",
                "def pandas_get_dataset(limit: float = -1, allFrames: bool = True):\n",
                "    files = os.listdir(path)\n",
                "    big_frame = pd.DataFrame()\n",
                "\n",
                "    if not allFrames:\n",
                "        files = [files[0]]\n",
                "\n",
                "    for f in files:\n",
                "        if f.endswith('.csv'):\n",
                "            frame = pd.read_csv(filepath_or_buffer=path +\n",
                "                                '/' + f, usecols=columns_to_get)\n",
                "            if limit != -1:\n",
                "                frame = frame.sample(n=limit, replace=False)\n",
                "            big_frame = pd.concat([big_frame, frame])\n",
                "    return big_frame\n",
                "\n",
                "\n",
                "def pandas_remove_extra_columns(index: str, data: pd.DataFrame) -> pd.DataFrame:\n",
                "\n",
                "    start_time = tm.time()\n",
                "    oppositeIndex = 'DIVERTED' if index == 'CANCELLED' else 'CANCELLED'\n",
                "    data.drop(oppositeIndex, axis=1, inplace=True)\n",
                "\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Extra column removal concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_balance_dataframe(data: pd.DataFrame, index: str) -> pd.DataFrame:\n",
                "    start_time = tm.time()\n",
                "    irregulars = data.query(index + ' == 1')\n",
                "    regulars = data.query(index + ' == 0')\n",
                "    \n",
                "    result = pd.concat([regulars.sample(len(irregulars)), irregulars]).sample(frac=1)\n",
                "    finish_time = tm.time() - start_time\n",
                "    print_and_save_time(\"Dataset balancing concluded: \" +\n",
                "                        str(finish_time) + \" seconds\")\n",
                "    return result\n",
                "\n",
                "\n",
                "def pandas_convert_names_into_numbers(data: pd.DataFrame) -> pd.DataFrame:\n",
                "\n",
                "    def str_to_float(s: str):\n",
                "        encoding = \"utf-8\"\n",
                "        b = s.encode(encoding)\n",
                "        return float(crc32(b) & 0xffffffff) / 2**32\n",
                "\n",
                "    for c in names_columns_to_convert:\n",
                "        data[c] = data[c].apply(str_to_float)\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_convert_dates_into_numbers(data: pd.DataFrame) -> pd.DataFrame:\n",
                "    multiplier: float = 1 / 365\n",
                "\n",
                "    def date_to_day_of_year(date_string) -> float:\n",
                "        date = dt.strptime(date_string, \"%Y-%m-%d\")\n",
                "        day = date.timetuple().tm_yday - 1\n",
                "        return day * multiplier\n",
                "\n",
                "    for i in date_columns_to_convert:\n",
                "        data[i] = data[i].apply(date_to_day_of_year)\n",
                "\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_convert_times_into_numbers(data: pd.DataFrame) -> pd.DataFrame:\n",
                "\n",
                "    def time_to_interval(time) -> float:\n",
                "        t = int(float(time))\n",
                "        h = t // 100\n",
                "        m = t % 100\n",
                "        t = h * 60 + m\n",
                "        return float(t / 1140)\n",
                "\n",
                "    for c in time_columns_to_convert:\n",
                "        data[c] = data[c].apply(time_to_interval)\n",
                "\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_convert_distance_into_numbers(data: pd.DataFram) -> pd.DataFrame:\n",
                "\n",
                "    multiplier: float = float(1) / float(max_distance)\n",
                "\n",
                "    for c in numeric_columns_to_convert:\n",
                "        data[c] = data[c].apply(lambda x: x * multiplier)\n",
                "    return data\n",
                "\n",
                "\n",
                "def pandas_split_data(data: pd.DataFrame, label: str, k: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
                "    split_list = []\n",
                "\n",
                "    data.drop_duplicates(inplace=True)\n",
                "\n",
                "    irregulars = data.query(label + ' == 1')\n",
                "    regulars = data.query(label + ' == 0')\n",
                "\n",
                "    k_elements_half_number = round((len(data) / k) / 2)\n",
                "\n",
                "    for i in range(1, k + 1):\n",
                "        k_irregulars_sample = irregulars.head(k_elements_half_number)\n",
                "        k_regulars_sample = regulars.head(k_elements_half_number)\n",
                "        k_sample = pd.concat([k_irregulars_sample, k_regulars_sample])\n",
                "\n",
                "        split_list.append(k_sample.to_numpy())\n",
                "        irregulars = irregulars.drop(k_irregulars_sample.index)\n",
                "        regulars = regulars.drop(k_regulars_sample.index)\n",
                "\n",
                "    return split_list\n",
                "\n",
                "def pandas_common_preprocess(data: ps.DataFrame | pd.DataFrame) -> ps.DataFrame | pd.DataFrame:\n",
                "\n",
                "    common_start_time = tm.time()\n",
                "\n",
                "    data.dropna(how='any', axis='index', inplace=True)\n",
                "\n",
                "    null_removal_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Null values removal concluded: \" +\n",
                "                        str(null_removal_finish_time) + \" seconds\")\n",
                "\n",
                "    pandas_balance_dataframe(data, problem_to_solve)\n",
                "\n",
                "    names_start_time = tm.time()\n",
                "    data = pandas_convert_names_into_numbers(data)\n",
                "    names_finish_time = tm.time() - names_start_time\n",
                "    print_and_save_time(\"Names conversion concluded: \" +\n",
                "                        str(names_finish_time) + \" seconds\")\n",
                "\n",
                "    dates_start_time = tm.time()\n",
                "    data = pandas_convert_dates_into_numbers(data)\n",
                "    dates_finish_time = tm.time() - dates_start_time\n",
                "    print_and_save_time(\"Dates conversion concluded: \" +\n",
                "                        str(dates_finish_time) + \" seconds\")\n",
                "\n",
                "    times_start_time = tm.time()\n",
                "    data = pandas_convert_times_into_numbers(data)\n",
                "    times_finish_time = tm.time() - times_start_time\n",
                "    print_and_save_time(\"Times conversion concluded: \" +\n",
                "                        str(times_finish_time) + \" seconds\")\n",
                "\n",
                "    distance_start_time = tm.time()\n",
                "    data = pandas_convert_distance_into_numbers(data)\n",
                "    distance_finish_time = tm.time() - distance_start_time\n",
                "    print_and_save_time(\"Distance conversion concluded: \" +\n",
                "                        str(distance_finish_time) + \" seconds\")\n",
                "\n",
                "    common_finish_time = tm.time() - common_start_time\n",
                "    print_and_save_time(\"Common preprocessing concluded: \" +\n",
                "                        str(common_finish_time) + \" seconds\")\n",
                "    return data\n",
                "\n",
                "if not check_preprocessed_data_exists():\n",
                "        download_dataset()\n",
                "\n",
                "        start_time = tm.time()\n",
                "        data = get_dataset(dataset_limit, use_all_dataset_frames)\n",
                "\n",
                "        finish_time = tm.time() - start_time\n",
                "        print_and_save_time(\"Dataset reading concluded: \" +\n",
                "                            str(finish_time) + \" seconds\")\n",
                "\n",
                "        data = pandas_common_preprocess(data)\n",
                "        pandas_save_dataset(data)\n",
                "else:\n",
                "    data = pandas_load_dataset()\n",
                "    \n",
                "data = pandas_remove_extra_columns(problem_to_solve, data)\n",
                "\n",
                "start_time = tm.time()\n",
                "preprocessing_splits = pandas_split_data(data, problem_to_solve, number_of_splits)\n",
                "\n",
                "finish_time = tm.time() - start_time\n",
                "print_and_save_time(\"Dataset splitting concluded: \" +\n",
                "                    str(finish_time) + \" seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generic Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    '''\n",
                "    Calculates the sigmoid of the given data\n",
                "    '''\n",
                "    g = 1.0 / (1.0 + np.exp(-x))\n",
                "    return g\n",
                "\n",
                "def binary_cross_entropy(y, y_label, w, l2):\n",
                "    '''\n",
                "    Calculates the binary cross entropy loss of the calculated y and the given y_label\n",
                "    '''\n",
                "    loss = -np.mean(y_label*(np.log(y)) + (1-y_label)\n",
                "                    * np.log(1-y)) + regularize(w, l2)\n",
                "    return loss\n",
                "\n",
                "def regularize(W, l2):\n",
                "    '''\n",
                "    Calculates the regularization term for the loss\n",
                "    '''\n",
                "    return (l2 / 2) * np.sum(np.square(W))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parallel Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ParallelLogisticRegression:\n",
                "    iterations: int\n",
                "    learning_rate: float\n",
                "    batch_size: int\n",
                "    l2: float\n",
                "    W: Broadcast\n",
                "    b: float\n",
                "\n",
                "def initialize(self: ParallelLogisticRegression, size):\n",
                "    self.W = context.broadcast(np.random.rand(size))\n",
                "    self.b = np.random.rand()\n",
                "\n",
                "\n",
                "def parallel_train(self: ParallelLogisticRegression, data: ps.DataFrame):\n",
                "    initialize(self, len(data.columns) - 1)\n",
                "\n",
                "    num_chunks = X.count() // self.batch_size\n",
                "    chunk_percent = 1/num_chunks\n",
                "\n",
                "    batches = data.randomSplit([chunk_percent] * num_chunks)\n",
                "\n",
                "    Y_labels = [b.select(problem_to_solve).rdd for b in batches]\n",
                "    X = [b.drop(problem_to_solve).rdd for b in batches]\n",
                "\n",
                "    losses = []\n",
                "    gradients = []\n",
                "\n",
                "    for _ in range(self.iterations):\n",
                "        _losses = []\n",
                "        _gradients = [] \n",
                "\n",
                "        for b_X, b_Y_labels in zip(X, Y_labels):\n",
                "            Y = self.evaluate(b_X)\n",
                "            _losses.append(binary_cross_entropy(\n",
                "                Y, b_Y_labels, self.W, self.l2))\n",
                "            (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "            _gradients.append(dW)\n",
                "            self.update(dW, db)\n",
                "        losses.append(np.mean(_losses))\n",
                "        gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)\n",
                "\n",
                "def parallel_evaluate(self: ParallelLogisticRegression, X: RDD):\n",
                "    Z = X.map(lambda x: np.dot(x, self.W.value)).reduce(lambda a, b: a+b + self.b)\n",
                "    Z = Z.map(lambda x: sigmoid(x))\n",
                "    return Z\n",
                "        \n",
                "def update(self, dW: list[float], db: float):\n",
                "        self.W = context.broadcast(self.W.value - self.learning_rate * dW)\n",
                "        self.b = self.b - self.learning_rate * db"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Serial Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SerialLogisticRegression():\n",
                "    def __init__(self, iterations: int, learning_rate: float, batch_size: int, l2: float):\n",
                "        self.iterations = iterations\n",
                "        self.learning_rate = learning_rate\n",
                "        self.batch_size = batch_size\n",
                "        self.l2 = l2\n",
                "\n",
                "    def initialize(self, columns_number):\n",
                "        self.W = np.random.rand(columns_number)\n",
                "        self.b = np.random.rand()\n",
                "\n",
                "    def evaluate(self, X):\n",
                "        Z = np.dot(X, self.W) + self.b\n",
                "        Z = sigmoid(Z)\n",
                "        return Z\n",
                "\n",
                "    def gradient(self, X, Y, Y_label):\n",
                "        '''\n",
                "        Calculates the gradient w.r.t weights and bias\n",
                "        '''\n",
                "\n",
                "        # Number of training examples.\n",
                "        m = X.shape[0]\n",
                "\n",
                "        # Gradient of loss w.r.t weights with regularization\n",
                "        dw = (1/m)*np.dot(X.T, (Y - Y_label)) + self.l2 * self.W\n",
                "\n",
                "        # Gradient of loss w.r.t bias with regularization\n",
                "        db = (1/m)*np.sum((Y - Y_label))\n",
                "\n",
                "        return dw, db\n",
                "\n",
                "    def update(self, dW, db):\n",
                "        self.W = self.W - self.learning_rate * dW\n",
                "        self.b = self.b - self.learning_rate * db\n",
                "\n",
                "    def train(self, X, Y_labels, iterations = 10):\n",
                "        self.initialize(X.shape[1])\n",
                "        losses = []\n",
                "        gradients = []\n",
                "\n",
                "        for _ in range(iterations):\n",
                "            _losses = []\n",
                "            _gradients = []\n",
                "            for b in range(X.shape[0]//self.batch_size):\n",
                "                b_X = X[b*self.batch_size:b*self.batch_size+self.batch_size, :]\n",
                "                b_Y_labels = Y_labels[b*self.batch_size:b *\n",
                "                                      self.batch_size+self.batch_size]\n",
                "                Y = self.evaluate(b_X)\n",
                "                _losses.append(binary_cross_entropy(\n",
                "                    Y, b_Y_labels, self.W, self.l2))\n",
                "                (dW, db) = self.gradient(b_X, Y, b_Y_labels)\n",
                "                _gradients.append(dW)\n",
                "                self.update(dW, db)\n",
                "            losses.append(np.mean(_losses))\n",
                "            gradients.append(np.mean(_gradients))\n",
                "\n",
                "        return (losses, gradients)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiments\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_roc(labels, results, name):\n",
                "    labels_and_results = sorted(\n",
                "        list(zip(labels, map(lambda x: x, results))), key=lambda x: x[1])\n",
                "\n",
                "    labels_by_weights = np.array([k for (k, _) in labels_and_results])\n",
                "\n",
                "    length = labels_by_weights.size\n",
                "\n",
                "    true_positives = labels_by_weights.cumsum()\n",
                "\n",
                "    num_positive = true_positives[-1]\n",
                "\n",
                "    false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
                "\n",
                "    true_positives_rate = true_positives / num_positive\n",
                "    false_positives_rate = false_positives / (length - num_positive)\n",
                "\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
                "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
                "    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
                "    plt.plot(false_positives_rate, true_positives_rate,\n",
                "             color='#8cbfd0', linestyle='-', linewidth=3.)\n",
                "    plt.plot((0., 1.), (0., 1.), linestyle='--',\n",
                "             color='#d6ebf2', linewidth=2.)\n",
                "\n",
                "    plt.savefig('./data/{}_roc.png'.format(name))\n",
                "    fig.clear()\n",
                "    plt.close()\n",
                "\n",
                "def plot_loss_gradient(iterations, train_losses, gradients, name):\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.set_xlabel('Iterations')\n",
                "    ax.set_ylabel('Loss/Gradient')\n",
                "    ax.set_title(name)\n",
                "    ax.plot(range(iterations), train_losses, label='Loss')\n",
                "    ax.plot(range(iterations), gradients, label='Gradient')\n",
                "    ax.grid()\n",
                "    ax.legend()\n",
                "\n",
                "    fig.savefig(\"./data/{}.png\".format(name))\n",
                "    fig.clear()\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparamters Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(100, 0.001, 0),\n",
                            " (100, 0.001, 0.1),\n",
                            " (100, 0.001, 0.001),\n",
                            " (100, 0.01, 0),\n",
                            " (100, 0.01, 0.1),\n",
                            " (100, 0.01, 0.001),\n",
                            " (100, 0.1, 0),\n",
                            " (100, 0.1, 0.1),\n",
                            " (100, 0.1, 0.001),\n",
                            " (200, 0.001, 0),\n",
                            " (200, 0.001, 0.1),\n",
                            " (200, 0.001, 0.001),\n",
                            " (200, 0.01, 0),\n",
                            " (200, 0.01, 0.1),\n",
                            " (200, 0.01, 0.001),\n",
                            " (200, 0.1, 0),\n",
                            " (200, 0.1, 0.1),\n",
                            " (200, 0.1, 0.001),\n",
                            " (500, 0.001, 0),\n",
                            " (500, 0.001, 0.1),\n",
                            " (500, 0.001, 0.001),\n",
                            " (500, 0.01, 0),\n",
                            " (500, 0.01, 0.1),\n",
                            " (500, 0.01, 0.001),\n",
                            " (500, 0.1, 0),\n",
                            " (500, 0.1, 0.1),\n",
                            " (500, 0.1, 0.001)]"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grid = { 'iter': [100, 200, 500], 'lr': [0.001, 0.01, 0.1], 'l2': [0, 0.1, 0.001], 'batch_size': [0, 20]}\n",
                "\n",
                "params = list(itertools.product(*grid.values()))\n",
                "\n",
                "params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "class Evaluator:\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        self.iterations = iterations\n",
                "        self.lr = lr\n",
                "        self.l2 = l2\n",
                "        self.batch_size = batch_size\n",
                "\n",
                "    def train(self, data):\n",
                "        total_train_losses = []\n",
                "        total_test_losses = []\n",
                "        for i, fold in enumerate(data):\n",
                "            train_data = data[:i] + data[i + 1:]\n",
                "            test_data = fold\n",
                "            losses,  = self.train_impl(train_data)\n",
                "            total_train_losses.append(losses) \n",
                "            loss = self.test_impl(test_data)\n",
                "            total_test_losses.append(loss)\n",
                "        return total_train_losses, total_test_losses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ParallelModelEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = ParallelLogisticRegression(self.iterations, self.lr, self.batch_size)\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        return parallel_train(self.model, train_data)\n",
                "    \n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        value: RDD = parallel_evaluate(self.model, test_data)\n",
                "        return binary_cross_entropy(np.array(value.collect()), y_label, self.model.W.value, self.l2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SequentialEvaluator(Evaluator):\n",
                "    def __init__(self, iterations, lr, l2, batch_size):\n",
                "        super().__init__(iterations, lr, l2, batch_size)\n",
                "        self.model = SerialLogisticRegression(self.iterations, self.lr, self.batch_size)\n",
                "\n",
                "    def train_impl(self, train_data: ps.DataFrame):\n",
                "        pass\n",
                "\n",
                "    def test_impl(self, test_data: ps.DataFrame):\n",
                "        pass"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.5 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.5"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e51fdbc37c5adaa12a9ed97b50fdaf0ff46be7ee1a114cc405b6eaf0c36d1bf8"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
